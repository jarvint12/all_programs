{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aad6fe643101dc4ea53624773f2b38cc",
     "grade": false,
     "grade_id": "cell-06d639cce7633bb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 2: N-gram language models\n",
    "\n",
    "# Released: 26.1.2021\n",
    "# Deadline: 8.2.2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "086546aac7617ec3a4a8f517eb02138a",
     "grade": false,
     "grade_id": "cell-3bfa793aa2cd6b17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Overview\n",
    "After completing this assignment, you'll understand how statical language models can be estimated. You'll be able to evaluate them and to generate text using them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81b5c9b0b7d5f6388ac1a77df6b08152",
     "grade": false,
     "grade_id": "cell-eb34778ec1241a7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of contents\n",
    "\n",
    "* [Introduction](#intro)\n",
    "    * [Language models](#languagemodel)\n",
    "    * [N-gram language models](#ngramlm)\n",
    "* [Task 1: Counting N-grams](#task_1)\n",
    "    * [Step 1.1: Pad with sentence boundary symbols](#subtask_1_1)\n",
    "    * [Step 1.2: Forming N-grams](#subtask_1_2)\n",
    "    * [Step 1.3: N-gram pipeline](#subtask_1_3)\n",
    "    * [Step 1.4: Count N-grams](#subtask_1_4)\n",
    "    * [Step 1.5: Full counting pipeline](#subtask_1_5)\n",
    "* [Task 2: Estimating an N-gram language model](#task_2)\n",
    "    * [Step 2.1: MLE](#subtask_2_1)\n",
    "    * [Step 2.2: Smoothing](#subtask_2_2)\n",
    "* [Task 3: Evaluating a language model](#task_3)\n",
    "    * [Step 3.1: Perplexity](#subtask_3_1)\n",
    "* [Task 4: Sampling from a language model](#task_4)\n",
    "    * [Step 4.1: Generate text](#subtask_4_1)\n",
    "* [Task 5: Working with real data](#task_5)\n",
    "    * [Step 5.1: Limiting the vocabulary](#subtask_5_1)\n",
    "    * [Step 5.2: Counting pipeline on real data](#subtask_5_2)\n",
    "    * [Step 5.3: Generate sentences, comment on differences](#subtask_5_3)\n",
    "* [Checklist before submission](#checklist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c54d8ef48e9dfcd74685c4fa0294e619",
     "grade": false,
     "grade_id": "cell-ecfa9a14c6632972",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "## Language models <a class=\"anchor\" id=\"languagemodel\"></a>\n",
    "As you've already noticed in the first assignment, different language sequences are not equally likely to occur. We've only looked at word frequencies and at frequencies of letter sequences, but what if we want to estimate how probable it is to see some sentence? Well, for this purpose you'll need a **language model**.\n",
    "\n",
    "A **language model** predicts the following word (or other symbol) given the observed history.\n",
    "$P(w_i| w_{i−1} . . . w_0)$.\n",
    "**Language models** are useful, for example, in the task of speech recognition. They help to distinguish between homophones (words that sound the same but have different meanings), for example, _\"too\"_ and _\"two\"_ in _\"I love you too\"_ and _\"I love you two\"._\n",
    "\n",
    "### What is an n-gram language model? <a class=\"anchor\" id=\"ngramlm\"></a>\n",
    "An **n-gram language model** approximates the probability of a word given all the previous words by using only the conditional probability of the $N-1$ preceding words. This approach is based on the Markov assumption: the next word depends only on a fixed-size window of previous words and not on the whole history: $P(w_i| w_{i−1} . . . w_{i-h})$\n",
    "To use an n-gram language model, we need estimates of the probability of seeing a particular word given the recent\n",
    "history. For instance, in the 3-gram model, the history is 2 words.\n",
    "\n",
    "* 3-gram: (say hello **to**) \n",
    "* 2 words of history\n",
    "* 1 word for **prediction**\n",
    "\n",
    "\n",
    "The simplest way to estimate probabilities is **maximum likelihood estimation** (MLE). To get the MLE estimate of an n-gram model we get counts from a corpus and normalize these counts to lie between 0 and 1. In the case of bigram language model, when we want to get a probability of some particular bigram $P(w_n|w_{n−1})$, we’ll compute the count of the bigram $C(w_{n−1}w_n)$ and normalize it by the sum of all the bigrams that start with the same first word $w_{n−1}$. It is easy to notice that the sum of all bigram counts that have the same fist word is simply the unigram count for that word $w_{n−1}$. Thus, we get:\n",
    "\n",
    "$P(w_n|w_{n−1}) = \\frac{C(w_{n−1}w_n)}{C(w_{n−1})}$, where\n",
    "$C$ tells the number of occurrences in the training\n",
    "set.\n",
    "\n",
    "The probability of the entire word sequence can be computed using **the chain rule of probability**. And considering the bigram assumption, it is:\n",
    "\n",
    "$P(w_1^n) ≈ \\prod_{k=1}^{n}P(w_k|w_1^{k−1}) ≈ \\prod_{k=1}^{n}P(w_k|w_{k−1}) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "708ce33c988714fcc3dcff1a06721495",
     "grade": false,
     "grade_id": "cell-8f444a72729f4d30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 1 <a class=\"anchor\" id=\"task_1\"></a>\n",
    "## Counting N-grams \n",
    "### 1.1 Pad with sentence boundary symbols (1 Point) <a class=\"anchor\" id=\"subtask_1_1\"></a>\n",
    "\n",
    "When dealing with language, it is very important to know what words tend to start a sentence and what words tend to end it. To learn this with n-grams, we create special symbols for start-of-sentence  _\"&lt;s>\"_ and end-of-sentence _\"&lt;/s>\"_. As a pre-processing step, you need to pad your sentences with these symbols.\n",
    "\n",
    "The number of sentence start tokens depends on N, the N-gram order. At the start of the sentence, the only context should be sentence start - so we use N-1 sentence start tokens. One sentence-end token is enough for any N, because we only need to predict one.\n",
    "\n",
    "Write a function that takes a list of tokens and pads it with sentence boundary symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3da97d2581f14282da6e7d36c56efd34",
     "grade": false,
     "grade_id": "cell-98a624d48aba3931",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pad(tokens, n):\n",
    "    \"\"\"Takes an iterable of tokens and pads with sentence boundary symbols.\n",
    "    \n",
    "    Always adds sentence end symbols. \n",
    "    For unigram sequences, does not add sentence starts.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    tokens : list, tuple, iterable\n",
    "        sentence to be padded\n",
    "    n : int\n",
    "        the ngram order\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Input padded with sentence boundary symbols\n",
    "    \"\"\"\n",
    "    start = \"<s>\"\n",
    "    end = \"</s>\"\n",
    "    # Always return tuples, we don't want to modify the input in-place.\n",
    "    i=0\n",
    "    temp_token=()\n",
    "    while i<n-1:\n",
    "        temp_token+=(start,)\n",
    "        #tokens = (start,)+tuple(tokens)\n",
    "        i+=1\n",
    "    tokens=temp_token+tuple(tokens)+(end,)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9acd921271ca4641353014c3ab6764e",
     "grade": true,
     "grade_id": "cell-86c59d8df560ca01",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "assert_equal(pad(['a','b','c'], 2), ('<s>', 'a', 'b', 'c', '</s>'))\n",
    "assert_equal(pad(('a','b','c'), 2), ('<s>', 'a', 'b', 'c', '</s>'))\n",
    "assert_equal(pad(iter('abc'), 3), ('<s>', '<s>', 'a', 'b', 'c', '</s>'))\n",
    "assert_equal(pad(('a','b','c'), 1), ('a', 'b', 'c', '</s>'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c48e88368235863b33ff172dfcfba58a",
     "grade": false,
     "grade_id": "cell-a0f997ab8969c491",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 Forming N-grams (1 Point) <a class=\"anchor\" id=\"subtask_1_2\"></a>\n",
    "\n",
    "Now we create a function that takes a list of tokens and forms all the N-grams that it can. (A,B,C) can form the bigrams (A,B) and (B,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af9676e96b74f2e728162fe60a6aa4ad",
     "grade": false,
     "grade_id": "cell-81b21c671cf892a4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_n_grams(tokens, n):\n",
    "    \"\"\"Takes in a tuple of tokens and forms n-grams\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    tokens : tuple\n",
    "        Tokens to make ngrams from\n",
    "    n : int\n",
    "        The order of ngrams to make\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of tuples: all ngrams of the specified order.\n",
    "    \"\"\"\n",
    "    tuple_list=list()\n",
    "    i=0\n",
    "    while i<=len(tokens)-n:\n",
    "        temp_token=(tokens[i],)\n",
    "        k=1\n",
    "        while k<n:\n",
    "            temp_token+=(tokens[i+k],)\n",
    "            k+=1\n",
    "        tuple_list.append(temp_token)\n",
    "        i+=1\n",
    "    return(tuple_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36bb7234f7a907b5ffece8fee6193ffd",
     "grade": true,
     "grade_id": "cell-c249921375a3c149",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "assert_equal(make_n_grams(('a','b','c','d','e'), 2), [('a', 'b'), ('b', 'c'), ('c', 'd'), ('d', 'e')])\n",
    "assert_equal(make_n_grams(('a','b','c','d','e'), 3), [('a', 'b', 'c'), ('b', 'c', 'd'), ('c', 'd', 'e')])\n",
    "assert_equal(make_n_grams(('a','b','c','d','e'), 1), [('a',), ('b',), ('c',), ('d',), ('e',)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da068c9ef580b4c8dca4d506546df53a",
     "grade": false,
     "grade_id": "cell-29df50cc406e852f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 N-gram pipeline (1 Point) <a class=\"anchor\" id=\"subtask_1_3\"></a>\n",
    "\n",
    "Now we can integrate all the functions so far into an N-gram iterating pipeline. The pipeline produces all possible N-grams up to order N.\n",
    "\n",
    "In this task, the function is already implemented for you, so you don't need to implement anything here. However, the function combines your previous implementations, so it acts as an additional test. \n",
    "\n",
    "We will use this pattern multiple times in this assignment: integration is already implemented for you, so that the tasks that you need to implement don't depend on anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "359b5cf66bb2dcc54d3be96dab169955",
     "grade": false,
     "grade_id": "cell-25b108227c537bec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def allgrams_pipeline(data, max_n):\n",
    "    \"\"\"Produces ngrams of all orders up to max_n from data, with padding\n",
    "    \n",
    "    This uses the user defined pad() and make_n_grams() functions.\n",
    "    It acts as an additional test for those.\n",
    "    \n",
    "    However, you must not change this. If there is some error, change \n",
    "    pad or make_n_grams instead.\"\"\"\n",
    "    for sentence in data:\n",
    "        for n in range(1, max_n+1):\n",
    "            padded = pad(sentence, n)\n",
    "            yield from make_n_grams(padded, n)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2a9ef34b101994e7d12047fb4275c3e",
     "grade": true,
     "grade_id": "cell-2ac52b900b6e34f1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Small sanity check for allgrams_pipeline:\n",
    "iterator = allgrams_pipeline([[1,2,3,],[3]], 2)\n",
    "answers = [(1,), (2,), (3,), \n",
    "           (\"</s>\",), (\"<s>\", 1), (1, 2), (2, 3), (3, \"</s>\"), \n",
    "           (3,), (\"</s>\",), \n",
    "           (\"<s>\", 3), (3, \"</s>\")]\n",
    "for pipeline_produced, answer in zip(iterator, answers):\n",
    "    assert_equal(pipeline_produced, answer)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c42db4cb33009475fd7c8dcdb88f3e16",
     "grade": false,
     "grade_id": "cell-f6809f85a87bc520",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.4 Count N-grams (3 Points) <a class=\"anchor\" id=\"subtask_1_4\"></a>\n",
    "Now we can collect statistics from a whole corpus of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5618a60dc061148b5e4d3a8216d62ae",
     "grade": false,
     "grade_id": "cell-83cb415fd86ec1b4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_counts(ngrams, max_n):\n",
    "    \"\"\"Counts ngrams in a dataset.\n",
    "    \n",
    "    Takes an iterable of ngrams, of variable order. Simply counts how many times each\n",
    "    ngram is seen. The main idea is how the counts are organized.\n",
    "    \n",
    "    The input is an iterable, which might produce a stream such as:\n",
    "    \n",
    "    ('this',),\n",
    "    ('is',), \n",
    "    ('the',), \n",
    "    ('first',), \n",
    "    ('sentence',),\n",
    "    ('</s>',),    \n",
    "    ('<s>', 'this'), \n",
    "    ('this', 'is'), \n",
    "    ('is', 'the'), \n",
    "    ('the', 'first'), \n",
    "    ('first', 'sentence'), \n",
    "    ('sentence', '</s>'),\n",
    "\n",
    "    Note how the stream has a mix of unigrams and bigrams.\n",
    "\n",
    "    The output is a triply nested dict.\n",
    "    The first level is indexed by ngram order,\n",
    "    the second level is indexed by the history,\n",
    "    and the third level is indexed by the last token (the predicted token).\n",
    "    Additionally, we recommend making the third level an extended type of dict: a Counter\n",
    "    See https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "    Example of the output structure:\n",
    "    {\n",
    "        1: {\n",
    "            (,): \n",
    "                Counter({\n",
    "                    '<s>': 21,\n",
    "                    'this': 43,\n",
    "                    'most': 31,\n",
    "                    'is': 50,\n",
    "                })\n",
    "        2: {\n",
    "            ('<s>', ): \n",
    "                Counter({\n",
    "                    'this': 21,\n",
    "                }),\n",
    "            ('the',):\n",
    "                Counter({\n",
    "                    'most': 31,\n",
    "                    'least': 14,\n",
    "                }),\n",
    "        3: {\n",
    "            ('<s>', 'this'): \n",
    "                Counter({\n",
    "                    'is': 12,\n",
    "                    'has': 8,\n",
    "                    '</s>': 1,\n",
    "                }),\n",
    "            ('the', 'most'):\n",
    "                Counter({\n",
    "                    'beautiful': 8,\n",
    "                    'intelligent': 10,\n",
    "                    'funny': 3,\n",
    "                }),\n",
    "    }\n",
    "    This structure is useful, since each history will also get its own \n",
    "    conditional probability distribution.\n",
    "    Note that when n==1, the ngram history simply becomes \n",
    "    the empty tuple, (,). This is fine.\n",
    "  \n",
    "    Arguments\n",
    "    ---------\n",
    "    sentences : iterable (such as list)\n",
    "        An iterable over ngrams.\n",
    "    max_n : int\n",
    "        The maximum ngram order.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Triply nested dict, from ngram order to n_gram history parts, \n",
    "        to a dictionary of all continuations and their counts, e.g.\n",
    "        {2: {('a',): {'b': 3 'c': 4}}}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_gram_dict = {order: defaultdict(Counter) for order in range(1,max_n+1)}\n",
    "    # The line above creates the triply nested dict.\n",
    "    # The second and third layers are special: defaultdict and Counter\n",
    "    # See their documentation:\n",
    "    # https://docs.python.org/3/library/collections.html#collections.defaultdict\n",
    "    # https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "    \n",
    "    for ngram in iter(ngrams):\n",
    "        n_gram_dict[len(ngram)][ngram[0:len(ngram)-1]][ngram[-1]]+=1\n",
    "    \n",
    "    # Lastly, make the defaultdicts into normal dicts, \n",
    "    # so that defaultdict doesn't bite us later (it can hide some bugs)\n",
    "    return {n: dict(counts) for n, counts in n_gram_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcf5359b2eaca44682f198b91efc4f98",
     "grade": true,
     "grade_id": "cell-a2b258487651966d",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "                (\"say\",), \n",
    "                (\"hello\",), \n",
    "                (\"to\",), \n",
    "                (\"my\",),\n",
    "                (\"little\",),\n",
    "                (\"friend\",),\n",
    "                (\"</s>\",),\n",
    "                (\"<s>\", \"say\"), \n",
    "                (\"say\", \"hello\"), \n",
    "                (\"hello\", \"to\"), \n",
    "                (\"to\", \"my\"),\n",
    "                (\"my\", \"little\"),\n",
    "                (\"little\", \"friend\"),\n",
    "                (\"friend\", \"</s>\"),\n",
    "                (\"<s>\", \"say\", \"hello\"), \n",
    "                (\"say\", \"hello\", \"to\"), \n",
    "                (\"hello\", \"to\", \"my\"), \n",
    "                (\"to\", \"my\", \"little\"),\n",
    "                (\"my\", \"little\", \"friend\"),\n",
    "                (\"little\", \"friend\", \"</s>\"),\n",
    "                (\"say\",),\n",
    "                (\"hello\",),\n",
    "                (\"</s>\",),\n",
    "                (\"<s>\", \"say\"),\n",
    "                (\"say\", \"hello\"),\n",
    "                (\"hello\", \"</s>\"),\n",
    "                (\"<s>\", \"say\", \"hello\"),\n",
    "                (\"say\", \"hello\", \"</s>\"),\n",
    "                (\"say\",),\n",
    "                (\"it\",),\n",
    "                (\"to\",),\n",
    "                (\"my\",),\n",
    "                (\"hand\",),\n",
    "                (\"</s>\",),\n",
    "                (\"<s>\", \"say\"),\n",
    "                (\"say\", \"it\"),\n",
    "                (\"it\", \"to\"),\n",
    "                (\"to\", \"my\"),\n",
    "                (\"my\", \"hand\"),\n",
    "                (\"hand\", \"</s>\"),\n",
    "                ('<s>', '<s>', 'say'),\n",
    "                (\"<s>\", \"say\", \"it\"),\n",
    "                (\"say\", \"it\", \"to\"),\n",
    "                (\"it\", \"to\", \"my\"),\n",
    "                (\"to\", \"my\", \"hand\"),\n",
    "                (\"my\", \"hand\", \"</s>\"),\n",
    "]\n",
    "\n",
    "dummy_model = get_counts(dummy_corpus, 3)\n",
    "assert_equal(dummy_model[3][('<s>', 'say')], {'hello': 2, 'it': 1})\n",
    "assert_equal(dummy_model[3][('my', 'hand')], {'</s>': 1})\n",
    "assert_equal(dummy_model[2][('my',)], {'little': 1, 'hand': 1})\n",
    "assert_equal(dummy_model[2][('<s>',)], {'say': 3})\n",
    "assert_equal(dummy_model[2][('<s>',)], {'say': 3})\n",
    "assert '<s>' not in dummy_model[1][tuple()]\n",
    "assert dummy_model[1][tuple()]['say'] == 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a67bd57bbcc295f2c2f4b9a333dc5adc",
     "grade": false,
     "grade_id": "cell-194baaabe7d23274",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.5 Full counting pipeline (1 Point) <a class=\"anchor\" id=\"subtask_1_5\"></a>\n",
    "\n",
    "Now we can integrate everything so far: the N-gram pipeline and the counting function. Put together, these take a tokenized corpus and produce counts of all possible N-grams upto order N.\n",
    "They are designed to work together:\n",
    "```python\n",
    "get_counts(allgrams_pipeline(corpus, N), N)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "176ab30102476a02a3744c6e27def594",
     "grade": true,
     "grade_id": "cell-45edf0af6faa8057",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Putting everything so far together:\n",
    "\n",
    "dummy_data = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "              [\"say\", \"hello\"],\n",
    "              [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "dummy_model = get_counts(allgrams_pipeline(dummy_data, 3), 3)\n",
    "assert_equal(dummy_model[3][('<s>', 'say')], {'hello': 2, 'it': 1})\n",
    "assert_equal(dummy_model[3][('my', 'hand')], {'</s>': 1})\n",
    "assert_equal(dummy_model[2][('my',)], {'little': 1, 'hand': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d1f16d32e92ece88b69cd45b2c3d333",
     "grade": false,
     "grade_id": "cell-8f605b50c75c642c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 2 <a class=\"anchor\" id=\"task_2\"></a>\n",
    "## Estimating an N-gram language model \n",
    "\n",
    "Now that we can count N-grams, we'll start estimating language models with them. In practice this means taking the counts and producing probability estimates.\n",
    "\n",
    "### 2.1 MLE (3 Points)  <a class=\"anchor\" id=\"subtask_2_1\"></a>\n",
    "\n",
    "We'll look at the **Maximum Likelihood Estimate** for bigrams. In general case it works like this:\n",
    "\n",
    "$P(w_n|w^{n−1}_{n−N+1}) = \\frac{C(w^{n−1}_{n−N+1}w_n)}{C(w^{n−1}_{n−N+1})}$, where $C$ tells the number of occurrences in the training corpus. Basically, you just divide the count of a particular n-gram by the count of its context (history) part. \n",
    "\n",
    "* 3-gram: (say hello **to**) \n",
    "* 2 words of context\n",
    "* 1 word for **prediction**\n",
    "\n",
    "Here, the whole 3-gram is (say hello to), and its context part is (say hello). If some n-gram is absent from the training corpus, its MLE estimate would be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a66fcd80f85d37f5230465ee256e2159",
     "grade": false,
     "grade_id": "cell-c6305aa6ea606298",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Log-domain\n",
    "Before we get to computing though, there is one important thing to consider.\n",
    "\n",
    "Probabilities are small numbers, between 0 and 1, and they often need to be multiplied, which makes them smaller still. This quickly leads to numerical instability. In the case of n-gram language models, we encounter this when estimating a probability of a long sentence (we need to multiply many n-gram probabilities together). \n",
    "\n",
    "Typically probability computations are done in the log-domain instead. In the log-domain, multiplication becomes addition. This solves numerical stability, but also makes many formulas much simpler and faster.\n",
    "\n",
    "With log-probabilities, we define $log(0) = -\\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff6e3c5d3a97ac7fd43207c509c7670f",
     "grade": false,
     "grade_id": "cell-e6a1d8a030411e25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell has a utility function, which you need to use down the line.\n",
    "The function is already provided here because it is also needed for the\n",
    "sanity checks in the visible tests for the next task.\n",
    "\"\"\"\n",
    "\n",
    "def logsumexp2(*logs):\n",
    "    \"\"\"Linear-scale addition in log-scale\n",
    "    \n",
    "    https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\"\"\"\n",
    "    x_star = max(logs)\n",
    "    return x_star + log2(sum(pow(2, x-x_star) for x in logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc60f5e3bfe6b1716462ca6510f7441e",
     "grade": false,
     "grade_id": "cell-d7db46c1ece8df57",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "NEGINF = -float('inf')\n",
    "\n",
    "def logprob_mle(counts, context, token):\n",
    "    \"\"\"Produces Maximum Likelihood Estimate of log(P(token | context))\n",
    "    \n",
    "    The ngram counts are as produced by get_counts\n",
    "    Example of the model_counts structure:\n",
    "    {\n",
    "        1: {\n",
    "            (,): \n",
    "                {\n",
    "                    '<s>': 21,\n",
    "                    'this': 43,\n",
    "                    'most': 31,\n",
    "                    'is': 50,\n",
    "                })\n",
    "        2: {\n",
    "            ('<s>', ): \n",
    "                {\n",
    "                    'this': 21,\n",
    "                }),\n",
    "            ('the',):\n",
    "                {\n",
    "                    'most': 31,\n",
    "                    'least': 14,\n",
    "                }),\n",
    "        3: {\n",
    "            ('<s>', 'this'): \n",
    "                {\n",
    "                    'is': 12,\n",
    "                    'has': 8,\n",
    "                    '</s>': 1,\n",
    "                }),\n",
    "            ('the', 'most'):\n",
    "                {\n",
    "                    'beautiful': 8,\n",
    "                    'intelligent': 10,\n",
    "                    'funny': 3,\n",
    "                }),\n",
    "    }\n",
    "    \n",
    "    What should be done if the context has not been seen?\n",
    "    In this case, we have no definition for the distribution P(x | context).\n",
    "    Here we take the choice that we find the highest order of ngram for which\n",
    "    the context has been seen.\n",
    "\n",
    "    What should be done if the token has not been seen?\n",
    "    This can lead to a ValueError(\"math domain error\"), due to taking a\n",
    "    logarithm of 0. In this case, return NEGINF (defined above).\n",
    "    \n",
    "    TIP: If you find yourself with a division by zero error, you should instead\n",
    "    use logarithm identities to convert the division to substraction.\n",
    "    TIP2: You might be masking the above error if you use a try-except for NEGINF.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    counts : dict\n",
    "        Triply nested dict as shown above.\n",
    "    context : tuple\n",
    "        The context to predict on as tuple, e.g. ('<s>',)\n",
    "    token : str\n",
    "        The token to predict.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The log probabilty maximum likelihood estimate\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find an order where context has been seen: \n",
    "    n = len(context) + 1\n",
    "    if n not in counts or context not in counts[n]:\n",
    "        if n == 1:\n",
    "            raise ValueError(\"Invalid counts-dict, needs to have all lower order counts.\")\n",
    "        return logprob_mle(counts, context[1:], token)\n",
    "    else:\n",
    "        sum=0\n",
    "        for key in counts[n][context]:\n",
    "            sum+=counts[n][context][key]\n",
    "        try:\n",
    "            return log2(counts[n][context][token])-log2(sum)\n",
    "        except ValueError:\n",
    "            return NEGINF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfee4a612a7c349a7b236f2f96c4e697",
     "grade": false,
     "grade_id": "cell-37dd7cd686e85568",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell contains a premade N-gram count dict, in the same format\n",
    "as returned by get_counts(). This is used for a few tests below.\n",
    "Do not change this; changing this would only make it impossible for\n",
    "you to benefit from the provided visible tests.\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "TEST_COUNTS = {1: \n",
    "                    {\n",
    "                     tuple():\n",
    "                        Counter({'say': 3, \n",
    "                         'hello': 2, \n",
    "                         'to': 2, \n",
    "                         'my': 2, \n",
    "                         'little': 1, \n",
    "                         'friend': 1, \n",
    "                         '</s>': 3, \n",
    "                         'it': 1, \n",
    "                         'hand': 1})\n",
    "                    }, \n",
    "                2: \n",
    "                    {\n",
    "                     ('<s>',): \n",
    "                         Counter({'say': 3}), \n",
    "                     ('say',): \n",
    "                         Counter({'hello': 2, 'it': 1}), \n",
    "                     ('hello',): \n",
    "                         Counter({'to': 1, '</s>': 1}), \n",
    "                     ('to',): \n",
    "                         Counter({'my': 2}), \n",
    "                     ('my',): \n",
    "                         Counter({'little': 1, 'hand': 1}), \n",
    "                     ('little',): \n",
    "                         Counter({'friend': 1}), \n",
    "                     ('friend',): \n",
    "                         Counter({'</s>': 1}), \n",
    "                     ('it',): \n",
    "                         Counter({'to': 1}), \n",
    "                     ('hand',): \n",
    "                         Counter({'</s>': 1})\n",
    "                    }, \n",
    "                3: \n",
    "                    {\n",
    "                     ('<s>', '<s>'): \n",
    "                         Counter({'say': 3}), \n",
    "                     ('<s>', 'say'): \n",
    "                         Counter({'hello': 2, 'it': 1}), \n",
    "                     ('say', 'hello'): \n",
    "                         Counter({'to': 1, '</s>': 1}), \n",
    "                     ('hello', 'to'): \n",
    "                         Counter({'my': 1}), \n",
    "                     ('to', 'my'): \n",
    "                         Counter({'little': 1, 'hand': 1}), \n",
    "                     ('my', 'little'): \n",
    "                         Counter({'friend': 1}), \n",
    "                     ('little', 'friend'): \n",
    "                         Counter({'</s>': 1}), \n",
    "                     ('say', 'it'): \n",
    "                         Counter({'to': 1}), \n",
    "                     ('it', 'to'): \n",
    "                         Counter({'my': 1}), \n",
    "                     ('my', 'hand'): \n",
    "                         Counter({'</s>': 1})\n",
    "                    }\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c8f8069cd9dd6cf4e8be9ee2ba895ce",
     "grade": true,
     "grade_id": "cell-10fc716c6bb7dbfb",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "from collections import Counter\n",
    "\n",
    "assert_almost_equal(logprob_mle(TEST_COUNTS, (\"hello\",), \"</s>\"), -1.0, 2)\n",
    "assert_almost_equal(logprob_mle(TEST_COUNTS, (\"<s>\", \"say\"), \"it\"), -1.5850, 2)\n",
    "assert_almost_equal(logprob_mle(TEST_COUNTS, (\"<s>\",), \"say\"), 0., 2)\n",
    "assert_almost_equal(logprob_mle(TEST_COUNTS, (\"say\", \"it\"), \"friend\"), -float('inf'), 2)\n",
    "assert_almost_equal(logprob_mle(TEST_COUNTS, (\"hello\", \"to\"), \"my\"), 0., 2)\n",
    "# \"wow\" is never in context so this goes all the way to unigram:\n",
    "assert_almost_equal(logprob_mle(TEST_COUNTS, (\"say\", \"wow\"), \"say\"), -2.4150, 2)\n",
    "\n",
    "\n",
    "# Sanity check for proper distributions:\n",
    "# Probability distributions need to sum to 1 (which is 0. in log-domain)\n",
    "vocab = set(TEST_COUNTS[1][tuple()])\n",
    "test_contexts = [(\"hello\",), (\"<s>\",), (\"<s>\", \"say\"), (\"hello\", \"to\"), (\"say\", \"it\"), (\"say\", \"wow\")]\n",
    "for context in test_contexts:\n",
    "    all_log_probs = [logprob_mle(TEST_COUNTS, context, token)\n",
    "                    for token in vocab]\n",
    "    assert_almost_equal(logsumexp2(*all_log_probs), 0., 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ee032f9bb72cc4a07c48aa1239e7eab",
     "grade": false,
     "grade_id": "cell-eaee922237714ef5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2  Smoothing (5 Points)  <a class=\"anchor\" id=\"subtask_2_2\"></a>\n",
    "Simple N-gram models have one very serious limitation: they are unable to give a probability estimate not only for n-grams with new out-of-vocabulary words but also for the n-grams with known vocabulary but unseen during training. The higher the $N$, the sparser the data, and the more zero counts there will be. To overcome this problem, we need to redistribute some probability mass from more frequent events and give it to the events we’ve never seen. These techniques are called **smoothing**.\n",
    "\n",
    "In this assignment, you will implement **Absolute discounting** and **interpolation**. It is a decent smoothing method.\n",
    "\n",
    "First, we must remove some probability mass, which we can then redistribute. In this method, the probability mass is removed by subtracting an absolute amount,  $\\delta$, from each n-gram count. Since we remove an absolute number, rather than a proportion, this is called absolute discounting.\n",
    "\n",
    "Next, we add the removed probability mass back in by interpolating with a less-sparse distribution; in this case simply the lower order N-gram models. In the unigram case, we interpolate to the uniform distribution.\n",
    "\n",
    "$P(w_i|w^{i-1}_{i-n+1}) = \\frac{\\max(C(w_{i-n+1}^i)-\\delta,0)}{C(w^{i-1}_{i−n+1})} \n",
    "    + \\lambda(w^{i-1}_{i−n+1})P(w_i|w^{i-1}_{i-n+1}), n>1$\n",
    "    \n",
    "$P(w_i) = \\frac{\\max(C(w_i,0))-\\delta}{C(V)} \n",
    "    + \\lambda({0})\\frac{1}{|V|}, n=1$\n",
    "   \n",
    "$C(V)$ is the total unigram count (number of tokens in the data). $|V|$ is the size of the vocabulary.\n",
    "\n",
    "\n",
    "$\\lambda(x)$ is the interpolation weight, which is the total sum of discount deltas subtracted for this context, normalized by the context count (the denominator for this context). Mathematically:\n",
    "\n",
    "$\\lambda(w^{i}_{i−n+1}) = \n",
    "    \\frac{\\Sigma_xC(w_{i-n+1}^i w_x)-\\max(C(w_{i-n+1}^iw_x) -\\delta, 0)}\n",
    "    {C(w_{i-n+1}^i)}$\n",
    "\n",
    "$\\lambda({0}) = \n",
    "    \\frac{\\Sigma_xC(w_x-\\max(C(w_x) -\\delta, 0)}\n",
    "    {C(V)}$\n",
    "    \n",
    "Here, in the numerator, $C(w_{i-n+1}^i w_x)-\\max(C(w_{i-n+1}^iw_x) -\\delta, 0)$ is the delta that truly was subtracted from $C(w_{i-n+1}^i w_x)$. In an edge case where $C(w_{i-n+1}^i w_x) = 1$ and $\\delta=1.4$, this then comes out to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ea48c44d57a8ac8930a60ec0f1dcf80",
     "grade": false,
     "grade_id": "cell-d2c08bd2c805779f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "NEGINF = -float('inf')\n",
    "\n",
    "\n",
    "# Look at logprob_abs_discount first, to understand the full picture.\n",
    "# Then, start by implementing logprob_discounted\n",
    "# It has its own tests below; see that you can pass them first.\n",
    "# Next, implement log_interp_weight.\n",
    "# It also has its own tests.\n",
    "# Finally, fill in the missing parts in logprob_abs_discount\n",
    "\n",
    "def logprob_discounted(counts, context, token, delta):\n",
    "    \"\"\"The discounted log probability\n",
    "        \n",
    "    Remember to discount to 0 at most, max(count-delta, 0).\n",
    "    If discounted count becomes 0, the discounted log prob becomes -inf.\n",
    "    And the same concerns as with logprob_mle apply.\n",
    "    \n",
    "    This is the left side of the sum in the probability equations\n",
    "    (the log version of it).\n",
    "    \"\"\"\n",
    "    n = len(context) + 1  # N-gram order\n",
    "    tot_sum=sum(counts[n][context].values())\n",
    "    try:\n",
    "        return(log2(max(counts[n][context][token]-delta,0))-log2(tot_sum))\n",
    "    except ValueError:\n",
    "        return NEGINF\n",
    "    \n",
    "    #return(counts[n][context][token]-delta)\n",
    "\n",
    "def log_interp_weight(counts, context, delta):\n",
    "    \"\"\"The interpolation weight, as determined by the discount.\n",
    "    \n",
    "    You will need to figure out the total sum of discount applied\n",
    "    for this context.\n",
    "    \n",
    "    This is the lambda in the equations (log version of it).\n",
    "    \"\"\"\n",
    "    n = len(context) + 1  # N-gram order\n",
    "    discount_times=len(counts[n][context])\n",
    "    context_tot_count=sum(counts[n][context].values())\n",
    "    return(log2(len(counts[n][context]))+log2(delta)-log2(context_tot_count))\n",
    "    \n",
    "\n",
    "def logprob_abs_discount(counts, context, token, delta=0.2):\n",
    "    \"\"\"Produces smoothed estimate of log(P(token | context))\n",
    "    \n",
    "    Now we will use absolute discounting and interpolation to lower\n",
    "    orders.\n",
    "    \n",
    "    There are four main challenges to compute here:\n",
    "    1. The discounted count for the token\n",
    "        - Remember to discount to 0 at most, \n",
    "          max(count-delta, 0)\n",
    "        - If discounted count becomes 0, the discounted log prob becomes -inf.\n",
    "          And the same concerns as with logprob_mle apply.\n",
    "    2. The interpolation weight, as determined by the discount.\n",
    "        - You will need to figure out the total sum of discount applied\n",
    "          for this context.\n",
    "    3. The log probability to interpolate with.\n",
    "        - This is easy: use recursion. So just call:\n",
    "          logprob_abs_discount(counts, context[1:], token, delta)\n",
    "        - Unigrams are the special case: they interpolate with the uniform\n",
    "          distribution P(x) = 1 / vocab size.\n",
    "    4. Interpolation in the log domain.\n",
    "        - So you can get log(P_delta(token|context)) and \n",
    "          log(P_interp(token|context[1:])) without problems. But then you need\n",
    "          the logarithmic equivalent of a sum.\n",
    "        - For that, use the logsumexp2 function defined above. \n",
    "    \n",
    "    The ngram counts are as produced by get_counts, same format as \n",
    "    with logprob_mle.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    counts : dict\n",
    "        Triply nested dict as shown above.\n",
    "    context : tuple\n",
    "        The context to predict on as tuple, e.g. ('<s>',)\n",
    "    token : str\n",
    "        The token to predict.\n",
    "    delta : float\n",
    "        The value to discount by.\n",
    "    \"\"\"\n",
    "    n = len(context) + 1  # N-gram order\n",
    "    vocab = set(counts[1][tuple()])\n",
    "    V = len(vocab)  # Vocabulary size\n",
    "    \n",
    "    # Check that word is in the intended vocabulary,\n",
    "    # i.e. at least seen once in the data (as unigram).\n",
    "    # If the word is never seen in the data, we cannot expect it.\n",
    "    if token not in vocab:\n",
    "        return NEGINF\n",
    "\n",
    "    # Find an order where context has been seen:     \n",
    "    if n not in counts or context not in counts[n]:\n",
    "        if n == 1:\n",
    "            raise ValueError(\"Invalid counts-dict, needs to have all lower order counts.\")\n",
    "        return logprob_abs_discount(counts, context[1:], token)\n",
    "    \n",
    "    # 1. Discounted prob (computed by separate function above)\n",
    "    lp_discounted = logprob_discounted(counts, context, token, delta)\n",
    "    \n",
    "    # 2. Log interpolation weight (computed by separate function above):\n",
    "    log_lambda = log_interp_weight(counts, context, delta)\n",
    "    \n",
    "    # 3. Log lower order probability:\n",
    "    if n == 1:\n",
    "        # Stopping recursion at the unigram level, by interpolating with\n",
    "        # unigram distribution:\n",
    "        lp_lower = - log2(V)\n",
    "    else:  # Recursion\n",
    "        lp_lower = logprob_abs_discount(counts, context[1:], token)\n",
    "    \n",
    "    # 4. Putting it all together:\n",
    "    return(logsumexp2(lp_discounted,log_lambda+lp_lower))\n",
    "\n",
    "#print(logprob_abs_discount(TEST_COUNTS, (\"hello\",), \"</s>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "930abb7a9607e42a0cbdf6ea3a3757a6",
     "grade": true,
     "grade_id": "cell-1d242643db93675b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "# First we test logprob_discounted\n",
    "# Note that logprob_discounted is not a proper distribution!\n",
    "\n",
    "assert_almost_equal(logprob_discounted(TEST_COUNTS, (\"hello\",), \"</s>\", 0.2), -1.3219, 2)\n",
    "assert_almost_equal(logprob_discounted(TEST_COUNTS, (\"<s>\", \"say\"), \"it\", 0.2), -1.9069, 2)\n",
    "assert_almost_equal(logprob_discounted(TEST_COUNTS, (\"<s>\",), \"say\", 0.2), -0.0995, 2)\n",
    "assert_almost_equal(logprob_discounted(TEST_COUNTS, (\"say\", \"it\"), \"friend\", 0.2), -float('inf'), 2)\n",
    "assert_almost_equal(logprob_discounted(TEST_COUNTS, (\"hello\", \"to\"), \"my\", 0.2), -0.3219, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7fe0410ba320a53c1739294ba398056",
     "grade": true,
     "grade_id": "cell-a52cfe7dd50b3a7c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "# Then we test log_interp_weight\n",
    "\n",
    "assert_almost_equal(log_interp_weight(TEST_COUNTS, (\"hello\",), 0.2), -2.3219, 2)\n",
    "assert_almost_equal(log_interp_weight(TEST_COUNTS, (\"<s>\", \"say\"), 0.2), -2.9069, 2)\n",
    "assert_almost_equal(log_interp_weight(TEST_COUNTS, (\"<s>\",), 0.2), -3.9069, 2)\n",
    "assert_almost_equal(log_interp_weight(TEST_COUNTS, (\"say\", \"it\"), 0.2), -2.3219, 2)\n",
    "assert_almost_equal(log_interp_weight(TEST_COUNTS, (\"hello\", \"to\"), 0.2), -2.3219, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0a0771688a2a3507f250c77ed7c9651",
     "grade": true,
     "grade_id": "cell-aec5c000ef8b5356",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "# Sanity check for proper distributions:\n",
    "vocab = set(TEST_COUNTS[1][tuple()])\n",
    "test_contexts = [(\"hello\",), (\"<s>\",), (\"<s>\", \"say\"), (\"hello\", \"to\"), (\"say\", \"it\"), (\"say\", \"wow\")]\n",
    "for context in test_contexts:\n",
    "    all_log_probs = [logprob_abs_discount(TEST_COUNTS, context, token)\n",
    "                    for token in vocab]\n",
    "    assert_almost_equal(logsumexp2(*all_log_probs), 0., 5)\n",
    "\n",
    "\n",
    "assert_almost_equal(logprob_abs_discount(TEST_COUNTS, (\"hello\",), \"</s>\"), -1.1926, 2)\n",
    "assert_almost_equal(logprob_abs_discount(TEST_COUNTS, (\"<s>\", \"say\"), \"it\"), -1.7210, 2)\n",
    "assert_almost_equal(logprob_abs_discount(TEST_COUNTS, (\"<s>\",), \"say\"), -0.0803, 2)\n",
    "assert_almost_equal(logprob_abs_discount(TEST_COUNTS, (\"say\", \"it\"), \"friend\"), -8.6439, 2)\n",
    "assert_almost_equal(logprob_abs_discount(TEST_COUNTS, (\"hello\", \"to\"), \"my\"), -0.0255, 2)\n",
    "# \"wow\" is never in context so this goes all the way to unigram:\n",
    "assert_almost_equal(logprob_abs_discount(TEST_COUNTS, (\"say\", \"wow\"), \"say\"), -2.4150, 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a04bc5d6d8437d48aa8d2f9733241c4c",
     "grade": false,
     "grade_id": "cell-14ebdff021b00d29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 3  <a class=\"anchor\" id=\"task_3\"></a>\n",
    "## How to evaluate a language model?\n",
    "\n",
    "The best way to evaluate a language model is to look at its performance in the intended application. Unfortunately, it is usually time-consuming to run the whole system just to test the language model parameters. Instead, we can measure how well an n-gram model predicts unseen data called the test set or test corpus. The higher the probability that the model assigns to the test set, the better this model performs.\n",
    "\n",
    "### 3.1 Perplexity (1 Point) <a class=\"anchor\" id=\"subtask_3_1\"></a>\n",
    "Perplexity measures the performance of a language model on a test set. Real-world perplexity values typically range from tens to hundreds, and lower is better. There are many ways to view perplexity, see for example [Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/3.pdf). One intuitive view is: if perplexity on a test set is X, the model matches the test data as well as choosing randomly from X tokens would.\n",
    "\n",
    "If our model assigns zero-probability to some P(Token | Context), but then Context->Token does appear in the test set, perplexity goes to infinity. This is because our model is _certain_ that Context->Token cannot occur. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82037db381e0ab09315d86f1a6ec245c",
     "grade": false,
     "grade_id": "cell-c66599e72eb40a48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Again, the integration is done for you. The implementation of perplexity below uses the log probability functions you have filled in above, and also pad() and make_n_grams(). There is nothing to change here, but the tests serve as additional tests for your implementations. If you get an error here, you probably need to change something in the previous tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "139bab8d3647f958f17754e4dc4e20e1",
     "grade": false,
     "grade_id": "cell-3d20eab8204b8cfc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def perplexity(test_data, model_counts, logprob_func, **lp_kwargs):\n",
    "    \"\"\"\n",
    "    Computes perplexity on the given test data with the given language model\n",
    "    (as specified by the counts and the logprob function).\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    test_data : list\n",
    "        List of lists of tokenized sentences.\n",
    "    model_counts : dict\n",
    "        Triply nested dict of ngram counts, as returned by get_counts()\n",
    "    logprob_func : function\n",
    "        Function with signature (counts, context, token), which returns the\n",
    "        log-probability of the token given the context.\n",
    "    **lp_kwargs : kwargs\n",
    "        Log prob key word arguments, passed to logprob_func\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The perplexity of the model on the test data.\n",
    "    \"\"\"\n",
    "    max_n = max(model_counts.keys())\n",
    "    total_log_prob = 0.\n",
    "    num_tokens = 0\n",
    "    for sentence in test_data:\n",
    "        padded = pad(sentence, max_n)\n",
    "        ngrams = make_n_grams(padded, max_n)\n",
    "        for *context, token in ngrams:\n",
    "            total_log_prob += logprob_func(model_counts, tuple(context), token, **lp_kwargs)\n",
    "            num_tokens += 1\n",
    "    ppl = pow(2, -total_log_prob / num_tokens)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfbe7457edae56604e5dc5c4da6ac0e9",
     "grade": true,
     "grade_id": "cell-214c2796f178e3c2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "test_data_1 = [(\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"), (\"say\", \"hello\")]\n",
    "test_data_2 = [(\"my\", \"little\", \"hand\"), (\"say\", \"it\", \"to\", \"my\", \"friend\")]\n",
    "\n",
    "assert_almost_equal(perplexity(test_data_1, TEST_COUNTS, logprob_mle), 1.3351, 2)\n",
    "assert_equal(perplexity(test_data_2, TEST_COUNTS, logprob_mle), float('inf'))\n",
    "\n",
    "assert_almost_equal(perplexity(test_data_1, TEST_COUNTS, logprob_abs_discount), 1.3683, 2)\n",
    "assert_almost_equal(perplexity(test_data_2, TEST_COUNTS, logprob_abs_discount), 8.9966, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae25715ab761855dac53b9931cad6baf",
     "grade": false,
     "grade_id": "cell-05d07876b4ed295e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 4 <a class=\"anchor\" id=\"task_4\"></a>\n",
    "## Sampling from a language model\n",
    "### 4.1 Generate text (Not graded) <a class=\"anchor\" id=\"subtask_4_1\"></a>\n",
    "Another useful way to make a sanity check of how a model is performing, is by generating sentences with it.\n",
    "The procedure is about as follows:\n",
    "\n",
    "1. Start with the appropriate number (n-1) of start symbols \"&lt;s>\", e.g. (\"&lt;s>\",\"&lt;s>\") for trigrams. Optionally, some seed text can be given instead.\n",
    "2. Generate tokens one at a time. Given the context, you get a probability distribution over the next word. Sample from that distribution.\n",
    "3. The generated tokens become context for the next token to be generated.\n",
    "4. Stop generating when the end symbol is produced.\n",
    "\n",
    "NOTE: Although it should be possible to make the pseudorandom behaviour reproducible by setting a seed,\n",
    "    we were not satisfied with the robustness of that solution. Therefore, the generation functions are not\n",
    "    autograded. However, they are used later when working with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d8cc9015fc0938eef0be8787796a979",
     "grade": false,
     "grade_id": "cell-8abbd59bb4c0e2b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_text(model_counts, logprob_func, seed_text=None, **lp_kwargs):\n",
    "    \"\"\"Generates text from an N-gram model.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    model_counts : dict\n",
    "        N-gram counts as returned by get_counts()\n",
    "    logprob_func : callable\n",
    "        Function with signature (counts, context, token), which returns the\n",
    "        log-probability of the token given the context.\n",
    "    seed_text : list, optional\n",
    "        Text to start generating from. If None, will start from the\n",
    "        appropriate amount of sentence-start symbols (N-1).\n",
    "    **lp_kwargs : kwargs\n",
    "        Log prob key word arguments, passed to logprob_func\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Sentence generated by model as a list of tokens. If\n",
    "        seed_text was given, will include it. Padding is stripped.\n",
    "    \"\"\"\n",
    "    max_n = max(model_counts.keys())\n",
    "    vocab = list(model_counts[1][tuple()])\n",
    "    if seed_text is None:\n",
    "        seed_text = ('<s>',) * (max_n-1)\n",
    "    end = '</s>'\n",
    "    output = list(seed_text)\n",
    "    while output[-1] != end and len(output) < 200:  # Also guard against infinite loops\n",
    "        context = output[-max_n+1:] if max_n > 1 else []\n",
    "        token_logprobs = [2**logprob_func(model_counts, tuple(context), token, **lp_kwargs) \n",
    "                          for token in vocab]\n",
    "        next_part = random.choices(vocab, token_logprobs)\n",
    "        output.extend(next_part)  # next_part is a list: [token]\n",
    "    return tuple(token for token in output if token not in ['<s>', '</s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "240da1219a89fcbeb6979e9e7a7a8408",
     "grade": false,
     "grade_id": "cell-f3cbfe3afa729f52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 sentences from the MLE model:\n",
      "\tsay hello\n",
      "\tsay hello\n",
      "\tsay hello to my little friend\n",
      "\tsay hello to my little friend\n",
      "\tsay it to my little friend\n",
      "\tsay it to my hand\n",
      "\tsay it to my little friend\n",
      "\tsay it to my little friend\n",
      "\tsay hello to my hand\n",
      "\tsay it to my hand\n",
      "\n",
      "10 sentences from the Absolute discounted model:\n",
      "\tsay it say hello\n",
      "\tsay hello say hello to my little friend\n",
      "\tsay hello to my little friend\n",
      "\tsay say it to my little say hello little hello\n",
      "\tsay it to my say hello to my little friend\n",
      "\tsay hello to my hand\n",
      "\tsay hello to my little friend\n",
      "\tsay hello\n",
      "\tsay it to my hand\n",
      "\tsay hello to my hand\n"
     ]
    }
   ],
   "source": [
    "random.seed(23456)\n",
    "print(\"10 sentences from the MLE model:\")\n",
    "for _ in range(10):\n",
    "    print(\"\\t\"+\" \".join(generate_text(TEST_COUNTS, logprob_mle)))\n",
    "print()\n",
    "print(\"10 sentences from the Absolute discounted model:\")\n",
    "for _ in range(10):\n",
    "    print(\"\\t\"+\" \".join(generate_text(TEST_COUNTS, logprob_abs_discount)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e5177e4dee21e0db20960d6ac5f15b2",
     "grade": false,
     "grade_id": "cell-9eb98b5cfa7e7d04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 5 <a class=\"anchor\" id=\"task_5\"></a>\n",
    "## Working with real data\n",
    "\n",
    "Let's try to model some real data. In this assignment we are working with Jane Austen's novel Pride and Prejudice, which is provided by Project Gutenberg [here](http://www.gutenberg.org/ebooks/1342). Note that one single novel is actually a very small corpus, but nonetheless it works as an example.\n",
    "\n",
    "In the 01-text-processing assignment, we saw that text cannot be simply used raw. Instead the text is tokenized and processed to make it suitable for whatever application we have. In the coursedata directory, the novel we are working with has been lowercased, and tokenized, so that some (but not all) punctuation is retained. Each sentence in the novel is put on a separate line. E-book disclaimers and chapter markers have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93f0d0e28977352f5017cd5b0c27eab5",
     "grade": false,
     "grade_id": "cell-11c88e1929d9a7e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "janeausten_tokenized = []\n",
    "with open(\"/coursedata/02-ngrams/pride-and-prejudice-normalized-punct.txt\") as fi:\n",
    "    for line in fi:\n",
    "        janeausten_tokenized.append(line.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d75e965525fabfef29c5b7976f7a79c7",
     "grade": false,
     "grade_id": "cell-1d10ced8be06e2fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def detokenize(seq, full_sentence=True):\n",
    "    \"\"\"A simple rule-based detokenizer for this assignment\"\"\"\n",
    "    last_seen_start = full_sentence\n",
    "    formatted_tokens = []\n",
    "    for token in seq:\n",
    "        if token == \"<s>\":\n",
    "            last_seen_start = True\n",
    "            continue\n",
    "        if token == \"</s>\":\n",
    "            continue\n",
    "        if last_seen_start:\n",
    "            token = token.capitalize()\n",
    "            last_seen_start = False\n",
    "        if token == \"i\":\n",
    "            token = \"I\"\n",
    "        if token in \".!?,;\" or token == \"'s\":\n",
    "            formatted_tokens.append(token)\n",
    "        elif formatted_tokens:\n",
    "            formatted_tokens.append(\" \" + token)\n",
    "        else:\n",
    "            formatted_tokens.append(token)\n",
    "    return \"\".join(formatted_tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5f12a04beed90f2179e41d0d761c396",
     "grade": false,
     "grade_id": "cell-7a3fd27e99d82912",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see the first sentence of the book:\n",
      "It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's see the first sentence of the book:\")\n",
    "print(detokenize(janeausten_tokenized[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78bd813873caf69374b52ffae08256b9",
     "grade": false,
     "grade_id": "cell-9195a2c8bf79ed63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Train-test split\n",
    "\n",
    "We'll divide the data into training and test sets. The training set is used to create the model, and the test set used for evaluation. Here, we'll select the last 10% of the sentences to be used as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf3bf928f55d2d1bb329d02b432705cd",
     "grade": false,
     "grade_id": "cell-2bc1167478df6c70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has 5411 sentences, and the test set 601 sentences\n"
     ]
    }
   ],
   "source": [
    "test_split_index = round(0.9 * len(janeausten_tokenized))\n",
    "janeausten_train = janeausten_tokenized[:test_split_index]\n",
    "janeausten_test = janeausten_tokenized[test_split_index:]\n",
    "\n",
    "print(\"The training set has\", len(janeausten_train), \"sentences, and the test set\", len(janeausten_test), \"sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eeb071ed4ed22fade405e39e4ef20ecf",
     "grade": false,
     "grade_id": "cell-04de4ab989145dbb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Vocabulary size\n",
    "\n",
    "If we just use the full vocabulary of the training data, we will include some rare words, \n",
    "while excluding other words. One way to deal with this is to limit the vocabulary to some most common words. \n",
    "Everything else will be an unknown token, dealt with, together, as the \"&lt;UNK&gt;\" token.\n",
    "\n",
    "We will limit the vocabulary to the 2500 most common words in the training set. Additionally we include the end of sentence tag and the unknown word tag, because we want to predict them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1fec1435d26ae83dc276c5354c8eb87",
     "grade": false,
     "grade_id": "cell-e8f1e6a116027d92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most common tokens:\n",
      ",\n",
      ".\n",
      "the\n",
      "to\n",
      "and\n",
      "of\n",
      "her\n",
      "i\n",
      "a\n",
      "in\n",
      "\n",
      "The 2490-2500 most common tokens:\n",
      "thinks\n",
      "pompous\n",
      "promises\n",
      "writer\n",
      "punctual\n",
      "heavy\n",
      "stately\n",
      "grievous\n",
      "summons\n",
      "viewing\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "janeausten_unigram_counts = Counter(itertools.chain.from_iterable(janeausten_train)).most_common()\n",
    "\n",
    "print(\"The 10 most common tokens:\")\n",
    "print(\"\\n\".join(word for word, freq in janeausten_unigram_counts[:10]))\n",
    "print()\n",
    "print(\"The 2490-2500 most common tokens:\")\n",
    "print(\"\\n\".join(word for word, freq in janeausten_unigram_counts[2490:2500]))\n",
    "\n",
    "janeausten_vocab_filt = set(word for word, freq in janeausten_unigram_counts[:2500]) | {\"</s>\", \"<unk>\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52d57f48e084ea5fa6869471016390cd",
     "grade": false,
     "grade_id": "cell-2391bf7bd77a1703",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5.1 Limiting the vocabulary (1 Point) <a class=\"anchor\" id=\"subtask_5_1\"></a>\n",
    "\n",
    "Implement the replaing of tokens which are not in the vocabulary (out-of-vocabulary words, OOVs). They are to be replaced with the unknown token \"&lt;unk&gt;\". Fill in the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91acf388fa0fec987d51ddf454b4ce59",
     "grade": false,
     "grade_id": "cell-d02c0b6e2a7711d7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def replace_oovs(vocab, data, unk=\"<unk>\"):\n",
    "    \"\"\"Replace OOV words with unknown-token\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    vocab : set\n",
    "        The set of tokens that are in-vocabulary.\n",
    "        token not in vocab => token is out-of-vocabulary.\n",
    "    data : list of iterables\n",
    "        List of sentences, which are lists (or other iterables) of tokens.\n",
    "    unk : str\n",
    "        Token to replace tokens which are not in the vocabulary\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of lists, (list of sentences in data, sentences are lists of tokens)\n",
    "        The data with out-of-vocabulary tokens replaced with the unknown token.\n",
    "        Does NOT modify in-place.\n",
    "    \n",
    "    \"\"\"\n",
    "    # NOTE: Do not modify input in-place.\n",
    "    # YOUR CODE HERE\n",
    "    data_oovs_replaced=list()\n",
    "    for sentence in data:\n",
    "        sentence_list=list()\n",
    "        for token in sentence:\n",
    "            if not token in vocab:\n",
    "                sentence_list.append(unk)\n",
    "            else:\n",
    "                sentence_list.append(token)\n",
    "        data_oovs_replaced.append(sentence_list)\n",
    "    return data_oovs_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b5c4f8e9ccec6b501149513a4a57bea",
     "grade": true,
     "grade_id": "cell-4daf2001dce04789",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "assert_equal(replace_oovs({\"a\",\"b\",\"c\"}, [[\"a\", \"b\"],[\"a\",\"b\",\"c\",\"d\"]]), [['a', 'b'], ['a', 'b', 'c', '<unk>']])\n",
    "assert_equal(replace_oovs({\"a\",\"b\",\"c\"}, [(\"a\", \"b\"),(\"a\",\"b\",\"c\",\"d\")]), [['a', 'b'], ['a', 'b', 'c', '<unk>']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f86cc9a1954c8c57db6b8672ef907491",
     "grade": false,
     "grade_id": "cell-90bdfe5f57bd9bcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we'll apply the filter to the Jane Austen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03696320541d8e726c7bb48e4540a0cd",
     "grade": false,
     "grade_id": "cell-9af163c379d54e33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see an example:\n",
      "And do you <unk> it to either of those?\n"
     ]
    }
   ],
   "source": [
    "janeausten_train_filt = replace_oovs(janeausten_vocab_filt, janeausten_train)\n",
    "janeausten_test_filt = replace_oovs(janeausten_vocab_filt, janeausten_test)\n",
    "\n",
    "print(\"Let's see an example:\")\n",
    "print(detokenize(janeausten_train_filt[2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bce671545ee0950dbdbb501f18ba0474",
     "grade": false,
     "grade_id": "cell-e59be86cd2f39931",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5.2 Counting pipeline on real data (1 Point) <a class=\"anchor\" id=\"subtask_5_2\"></a>\n",
    "\n",
    "We'll estimate 5-gram counts from real data. Again, there is nothing for you to implement here, but this is an additional test for your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "109494ca3fc05ba1bb6d9812ed23bf25",
     "grade": true,
     "grade_id": "cell-29df9db997fc81fc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Produce counts from the real world data:\n",
    "janeausten_counts = get_counts(allgrams_pipeline(janeausten_train_filt, 5), 5)\n",
    "\n",
    "# Some more tests based on the real world data:\n",
    "assert_equal(janeausten_counts[3][(\"of\", \"the\")][\"house\"], 13)\n",
    "assert_equal(janeausten_counts[3][(\"at\", \"first\")][\".\"], 4)\n",
    "assert_equal(janeausten_counts[4][(\"at\", \"first\", \".\")][\"</s>\"], 4)\n",
    "assert_equal(janeausten_counts[5][(\"<s>\", \"<s>\", \"<s>\", \"<s>\")][\"oh\"], 73)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3508a734b1288f8019486692cd5a9ab3",
     "grade": false,
     "grade_id": "cell-9ea7418301cb5887",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "However, even though any unseen token is accounted for by the unknown token, we still get infinite perplexity. Some context-token pairs that occur in the test data have not been seen in the training data, so they get a zero probability estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87c73387cead85a1f7865a09574cdea1",
     "grade": false,
     "grade_id": "cell-f79809498aa30977",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(janeausten_test_filt, janeausten_counts, logprob_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4d22f97970fb20a2d772fa59c752745",
     "grade": false,
     "grade_id": "cell-c35757b74baa6caa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Smoothed model\n",
    "\n",
    "Now, we'll smooth our counts with a few different delta values and look at perplexities.\n",
    "\n",
    "Since the smoothed model is a bit more complex, and our Python implementation is not super efficient, we first wrap the log interpolation weight function in a caching class. This class does not change how log_interp_weight functions inside, but it just remembers the solutions that have already been computed, so we save on computing them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c54a2afc02493ddaef163d004d4da78",
     "grade": false,
     "grade_id": "cell-eb52281c654ac25d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LIWCache:\n",
    "    \"\"\"Very simple cache for log_interp_weight for speeding up querys\n",
    "    \n",
    "    The log_interp_weight function gets called many times with the same\n",
    "    arguments. The normal Python LRU Cache decorator however cannot handle\n",
    "    the counts argument, as it is unhashable. \n",
    "    \"\"\"\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "        self.cache = {}\n",
    "        self._caching = False\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        \n",
    "    def __call__(self, counts, context, delta):\n",
    "        if not self._caching:\n",
    "            return self.func(counts, context, delta)\n",
    "        key = (context, delta)\n",
    "        if key not in self.cache:\n",
    "            self.cache[key] = self.func(counts, context, delta)\n",
    "            self.misses += 1\n",
    "        else:\n",
    "            self.hits += 1\n",
    "        return self.cache[key]\n",
    "        \n",
    "    @property\n",
    "    def caching(self):\n",
    "        return self._caching\n",
    "\n",
    "    @caching.setter\n",
    "    def caching(self, value):\n",
    "        self._caching = value\n",
    "        if not value:\n",
    "            self.cache = {}  # Empty\n",
    "            self.hits = 0\n",
    "            self.misses = 0\n",
    "\n",
    "if not isinstance(log_interp_weight, LIWCache):\n",
    "    log_interp_weight = LIWCache(log_interp_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a71d1e4c6e91fd174f15cdee250f022",
     "grade": false,
     "grade_id": "cell-6ce3158bd7c64819",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, we can get some type of proper perplexity value. What is more, we can try to optimize the delta value for lower perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f001cb5ad12af42ddc8d4d61d6db385",
     "grade": false,
     "grade_id": "cell-fea939e1b5372885",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity with default smoothing:\n",
      "266.2130040868837\n",
      "Perplexity with higher smoothing:\n",
      "176.6217561833044\n"
     ]
    }
   ],
   "source": [
    "log_interp_weight.caching = True\n",
    "print(\"Perplexity with default smoothing:\")\n",
    "print(perplexity(janeausten_test_filt, janeausten_counts, logprob_abs_discount, delta=0.2))\n",
    "log_interp_weight.caching = False\n",
    "\n",
    "log_interp_weight.caching = True\n",
    "print(\"Perplexity with higher smoothing:\")\n",
    "print(perplexity(janeausten_test_filt, janeausten_counts, logprob_abs_discount, delta=5.2))\n",
    "log_interp_weight.caching = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ab67b0158639b76a5609f1373754624",
     "grade": false,
     "grade_id": "cell-1c8a5cc764ce91bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5.3 Generate sentences, comment on differences (3 Points) <a class=\"anchor\" id=\"subtask_5_3\"></a>\n",
    "\n",
    "Let's see what kinds of sentences do the two types of models generate. We'll seed the generation with the start of the novel, and see where the model goes.\n",
    "Generate some sentences, and comment on the differences.\n",
    "\n",
    "- Which model generates text, that is most similar to the original? Why?\n",
    "- Which model would be the best language model for e.g. optical character recognition of Jane Austen's hand written notes, and why?\n",
    "\n",
    "NOTE: Answer in the manually graded answer box below.\n",
    "\n",
    "NOTE2: Generating text with the smoothed model may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d57af6c0bfa969160721e3d07fe6adf",
     "grade": false,
     "grade_id": "cell-f7ae1841cd89c650",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "seed_text = ['it', 'is', 'a', 'truth', 'universally', 'acknowledged', ',', 'that',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "763a6a49992db731b0316e7172767293",
     "grade": false,
     "grade_id": "cell-37cc13550d54f074",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences from the MLE model:\n",
      "\n",
      "It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\n",
      "It is a truth universally acknowledged, that a single man in possession of a most valuable living, had it pleased the gentleman we were speaking of just now.\n",
      "It is a truth universally acknowledged, that a single man in possession of a most valuable living, had it pleased the gentleman we were speaking of just now.\n",
      "It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\n",
      "It is a truth universally acknowledged, that a single man in possession of a most valuable living, had it pleased the gentleman we were speaking of just now.\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences from the MLE model:\")\n",
    "print()\n",
    "for i in range(5):\n",
    "    print(detokenize(generate_text(janeausten_counts, logprob_mle, seed_text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c59c9a2231bfbd86d43720d4de6ef5d",
     "grade": false,
     "grade_id": "cell-1be4b0fb3303dfb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences from a smoothed model:\n",
      "It is a truth universally acknowledged, that a mistress might be found for it at this time, when <unk> to by any of those pleasures which too often console the unfortunate for their folly or their <unk>.\n",
      "It is a truth universally acknowledged, that the living became vacant two years ago, and quitted the house as they entered the woods, and <unk> her most <unk> to his wife he was very fond of them.\n",
      "It is a truth universally acknowledged, that though she received his attentions with pleasure, she did not suppose lydia to be <unk> <unk>.\n",
      "It is a truth universally acknowledged, that a person who could <unk> her.\n",
      "It is a truth universally acknowledged, that the match might be broken in on.\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences from a smoothed model:\")\n",
    "log_interp_weight.caching = True\n",
    "for i in range(5):\n",
    "    print(detokenize(generate_text(janeausten_counts, logprob_abs_discount, seed_text, delta=5.2)))\n",
    "log_interp_weight.caching = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6099b65fc5d1d07bb9116f06fcc7591c",
     "grade": true,
     "grade_id": "cell-a48c96187f0db2fb",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "MLE model generates sentences that are most closely to the original, because it overestimates the words, contexts and sentences that are seen in the original text, while the latter model does the smoothing and thus probabilities differ a bit from the original text. One could say, that the first model overfits to the data. I think the second model would be optical character recognition of Jane Austen's hand written notes, since it would recognize better words and structures, that did not occur in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43c0b9911c524adadd3eaa012e6b1c5e",
     "grade": false,
     "grade_id": "cell-316ac0753b1744f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checklist before submission <a class=\"anchor\" id=\"checklist\"></a>\n",
    "### 1\n",
    "To make sure that you didn't forget to import some package or to name some variable, press **Kernel -> Restart** and then **Cell -> Run All**. This way your code will be run exactly in the same order as during the autograding.\n",
    "### 2\n",
    "In this exercise, validation will not work well because the generation will take a long time. You should rely on running all as in **1**\n",
    "### 3\n",
    "To submit the notebook, click on the **jupyterhub** logo in the upper left part of the window, choose the **Assignments** folder, and press **submit**. You can submit multiple times, only the last one counts.\n",
    "### 4\n",
    "Please fill in the feedback form in the [Assignment](https://mycourses.aalto.fi/mod/questionnaire/view.php?id=693144) section of Mycoures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
