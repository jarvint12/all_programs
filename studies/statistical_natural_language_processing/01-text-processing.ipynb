{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b26c15f744e3963035af1999fbff2fc",
     "grade": false,
     "grade_id": "cell-3f2c101d987217b7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 1: Text Preprocessing\n",
    "\n",
    "## Released: 19.01.2021 at 14:30\n",
    "## Deadline: 01.02.2021 at 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "457695a4f4ceed6349c5c2c60a978c9c",
     "grade": false,
     "grade_id": "cell-a1a6fd2cf64b99ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Overview\n",
    "Consider this assignment an introduction to statistics of different language units (letters, pairs of letters, words). We will explore the frequency distribution of these different language units and then discuss what this knowledge might give. Moreover, we'll talk about how to handle raw text, how to separate it into different units and how those units and operations are called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04b054b5e3d7ea4cdbc5211561efb822",
     "grade": false,
     "grade_id": "cell-60bb5ff6140a29ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of contents\n",
    "\n",
    "* [Task 1: Letter and Letter pair Frequency analysis](#task_1)\n",
    "    * [Step 1.1: Prepare the text](#subtask_1_1)\n",
    "    * [Step 1.2: Get letter frequencies](#subtask_1_2)\n",
    "    * [Step 1.3: Letter frequency analysis](#subtask_1_3)\n",
    "    * [Step 1.4: Count all possible two-letter strings](#subtask_1_4)\n",
    "    * [Step 1.5: Get letter pair counts](#subtask_1_5)\n",
    "    * [Step 1.6: Letter pair frequency analysis](#subtask_1_6)\n",
    "* [Task 2: Word Tokenization](#task_2)\n",
    "    * [Step 2.1: Tokenize by whitespaces](#subtask_2_1)\n",
    "    * [Step 2.2: Tokenize with regular expressions](#subtask_2_2)\n",
    "    * [Step 2.3: Use Treebank tokenizer](#subtask_2_3)\n",
    "* [Task 3: ](#task_3)\n",
    "    * [Step 3.1: Analyse word frequencies](#subtask_3_1)\n",
    "    * [Step 3.2: Remove stop words](#subtask_3_2)\n",
    "    * [Step 3.3: How much is left without stop words?](#subtask_3_3) \n",
    "* [Checklist before submission](#checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f4c6ce5950ce5e8ca31e31e74858ad8",
     "grade": false,
     "grade_id": "cell-750a6466a51a9aeb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 1 <a class=\"anchor\" id=\"task_1\"></a>\n",
    "## Letter and Letter pair Frequency analysis\n",
    "\n",
    "The data used in this assignment is \"The Gold-Bug\" by Edgar Allan Poe. It is actually a story about the importance of letter frequencies. The narrator in the story was able to decipher a message leading to a hidden treasure by applying frequency analysis. The cipher used in the story is a substitute cipher where each letter is replaced by a different letter or number.\n",
    "\n",
    "Knowing the frequency of letters in a language is important not only for solving ciphers, but it also has practical applications like data compression. For example, Morse code uses the shortest symbols for the most frequent letters. \n",
    "\n",
    "In this task you'll need to discover for yourself the frequency distribution of single letters and of letter pairs in English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96f8a9ec899515c4313d5483e07c8c27",
     "grade": false,
     "grade_id": "cell-d5d46f8ec022767a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.1  <a class=\"anchor\" id=\"subtask_1_1\"></a>\n",
    "### Prepare the text (1 point)\n",
    "\n",
    "First of all, we need to load the text into the Jupyter Notebook and prepare it for further analysis. We will focus only on letters now and we don't care if they are capitalized or lowercased. \n",
    "\n",
    "Create a function that reads the data located in `/coursedata/01-text-processing/the_gold-bug.txt`, leaves only lowercased alphabetic characters in it, and writes them into a list as separate letters.\n",
    "\n",
    "HINT1: [string methods might come in handy](https://www.w3schools.com/python/python_ref_string.asp)\n",
    "\n",
    "HINT2: you can employ Python's open() and read() functions\n",
    "\n",
    "HINT3: Alphabetic characters are characters defined as “Letter” in the Unicode character database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7b4f72abd165d5e3414a695ae75f420",
     "grade": false,
     "grade_id": "cell-6027b855ab1ece6c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def read(file_name):\n",
    "    \"\"\"This function creates a list of lowercase alphabetic characters from a .txt file \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to the text file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lowercased_letters : list of strings\n",
    "        text as a list of lowercase letters\n",
    "    \"\"\"\n",
    "    lowercased_letters=list()\n",
    "    with open(file_name, 'r') as fr:\n",
    "        for line in fr:\n",
    "            for char in line:\n",
    "                if char.isalpha():# char!='\\n' and char!=' ' an:\n",
    "                    lowercased_letters.append(char.lower())\n",
    "        #[ch for ch in open('test.txt').read() if ch != '\\n' if ch != ' ']\n",
    "        #string=fr.read()#.replace('\\n', '')\n",
    "        #print(string)\n",
    "    #lowercased_letters=string.lower()\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    return lowercased_letters\n",
    "\n",
    "bug_letters = read('/coursedata/01-text-processing/the_gold-bug.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "635767646c0fca901ced32909b9f9b03",
     "grade": true,
     "grade_id": "cell-a6ece3ca5d559fd1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# checks if your function returns a list\n",
    "assert_equal(type(read('/coursedata/01-text-processing/the_gold-bug.txt')), list)\n",
    "\n",
    "# checks if your function returns a list of strings\n",
    "assert_equal(type(read('/coursedata/01-text-processing/the_gold-bug.txt')[0]), str)\n",
    "\n",
    "# checks if your list is of the right length\n",
    "assert_equal(len(read('/coursedata/01-text-processing/the_gold-bug.txt')), 58269)\n",
    "\n",
    "# checks if the strings in your list are alpabetic characters\n",
    "assert_equal(all([letter.isalpha() for letter in read('/coursedata/01-text-processing/the_gold-bug.txt')]), True)\n",
    "\n",
    "# checks if the strings in your list are lowercased\n",
    "assert_equal(all([letter.islower() for letter in read('/coursedata/01-text-processing/the_gold-bug.txt')]), True)\n",
    "\n",
    "# checks if your list has the first 10 members right \n",
    "assert_equal(read('/coursedata/01-text-processing/the_gold-bug.txt')[:10], \n",
    "             ['t', 'h', 'e', 'g', 'o', 'l', 'd', 'b', 'u', 'g'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0049753342aad8fa5ac1ee09b9a4a5b6",
     "grade": false,
     "grade_id": "cell-0dc5934ca1a0e8dc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.2 <a class=\"anchor\" id=\"subtask_1_2\"></a>\n",
    "### Get letter frequencies (3 points)\n",
    "\n",
    "Now we can count how many times each letter occurred in the story, and then turn these counts into Maximum Likelihood probability estimates of seeing each letter. \n",
    "\n",
    "To do so, write a function that takes in a list of letters and returns a dictionary with their frequencies relative to the size of the whole text: $ freq_x = \\frac{n_x}{N} $, where $n_x$ is the number of times a letter $x$ was seen, and $N$ is the total number of letters in the text. These relative frequencies are probability estimates for the letters in our text.\n",
    "\n",
    "HINT: you might find **nltk.FreqDist** or **collections.Counter** useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91edaad811e9fce2ae86d68ab9068832",
     "grade": false,
     "grade_id": "cell-a3e7f79d12bc50df",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def get_freq(letters):\n",
    "    \"\"\"This function computes MLE probabilities of letters\n",
    "    \n",
    "    Given a list of letters, this function should return a probability dictionary,\n",
    "    where letters are keys and their MLE probabilities are values\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    letters : list of strings\n",
    "        text as a list of only lowercase letters \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    letter_freq_dict : dictionary-like object\n",
    "        a frequency dictionary of letters where values are relative frequencies (MLE probabilities)\n",
    "    \"\"\"\n",
    "    letter_freq_dict=collections.Counter()\n",
    "    N=len(letters)\n",
    "    for letter in letters:\n",
    "        letter_freq_dict[letter]+=1/N\n",
    "    \n",
    "    return letter_freq_dict\n",
    "\n",
    "bug_probability_dict = get_freq(bug_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2cf7330c7193edea1d2d06e4b9d7816",
     "grade": true,
     "grade_id": "cell-a53de57ea983102e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "# checks if the alphabet length is 26\n",
    "assert_equal(len(get_freq(bug_letters)), 26)\n",
    "\n",
    "# checks if probability of all the letters equals one\n",
    "assert_almost_equal(sum(get_freq(bug_letters).values()), 1., 3)\n",
    "\n",
    "# checks if the algorithm is doing what it is supposed to be doing on a dummy example\n",
    "assert_equal(get_freq(['b','b','b','a','a','c']), {\"b\":3/6,\"a\":2/6,\"c\":1/6})\n",
    "\n",
    "# checks if the probability of 'e' is correct\n",
    "assert_almost_equal(get_freq(bug_letters)['e'], 0.13125332509567694, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8301c231d42932bb75299108454aaf6",
     "grade": false,
     "grade_id": "cell-247e03c8c52f0e3e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.3 <a class=\"anchor\" id=\"subtask_1_3\"></a>\n",
    "### Letter frequency analysis (1 point)\n",
    "The counts of letters in a text differ quite much. That means the probabilities of seeing each letter are also different.\n",
    "\n",
    "Using your frequency dictionary, answer the following questions:\n",
    "\n",
    "1. what is the probability of the most frequent letter? (0.3 points)\n",
    "2. what is the least probable letter? (0.3 points)\n",
    "3. what is the order of English letters according to their probability? Answer with a sorted string of letters starting with the most frequent one. (0.4 points)\n",
    "\n",
    "Type your answers in the cell below. You can create an additional cell to do the calculations if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32f7083b68c2c6029b8991d14a8c61b7",
     "grade": false,
     "grade_id": "cell-7f9e8133be37542f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etaoinshrdlucmfwpygbvkxjqz\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "sorted_list=sorted(bug_probability_dict.items(), key=lambda pair: pair[1], reverse=True)\n",
    "\n",
    "# put your answer to question 1 as a float number into the variable below\n",
    "# For example:\n",
    "# most_frequent_letter_prob = 0.123\n",
    "most_frequent_letter_prob = sorted_list[0][1] ## FILL IN THE ANSWER\n",
    "\n",
    "# put your answer to question 2 as a string into the variable below\n",
    "# For example:\n",
    "# least_probable_letter = 'a'\n",
    "least_probable_letter = sorted_list[0][0] ## FILL IN THE ANSWER\n",
    "\n",
    "# put your answer to question 3 as a string into the variable below\n",
    "# For example:\n",
    "# sorted_letters = 'abcdefghigklmnopqrstuvwxyz'\n",
    "sorted_letters=\"\"\n",
    "for char in sorted_list:\n",
    "    sorted_letters+=char[0]\n",
    "print(sorted_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2fa8dd98e9de235bfa26acadc3fbe2a",
     "grade": true,
     "grade_id": "cell-57d24f0b13fea3d7",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a float\n",
    "assert_equal(type(most_frequent_letter_prob), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0c01b73ca9119ace9b57266958a2d3d",
     "grade": true,
     "grade_id": "cell-8082c3b77391c5f9",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a string\n",
    "assert_equal(type(least_probable_letter), str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f4b4e98270e90ecbdc5e766c9f8891a",
     "grade": true,
     "grade_id": "cell-de8d57ffd847e5d6",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if you typed in a string of 26 letters\n",
    "assert_equal(type(sorted_letters), str)\n",
    "assert_equal(len(sorted_letters), 26)\n",
    "# checks if the 3rd most frequent letter is 'a'\n",
    "assert_equal(sorted_letters[2], 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1e61017e7553d6ec056efe885305175",
     "grade": false,
     "grade_id": "cell-ff1e21f485bb65ac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.4 <a class=\"anchor\" id=\"subtask_1_4\"></a>\n",
    "### Count all possible two-letter strings (1 point)\n",
    "Some combinations of language units are more likely than other too. You'll see it in a minute.\n",
    "\n",
    "There are 26 letters in English alphabet. How many possible two-letter strings are there according to combinatorics ('permutations' combinatorically speaking)? For example, if we have an alphabet of 3 letters **a**, **b** and **c**. We can have 9 two-letter strings: **aa**, **bb**, **cc**, **ab**, **ac**, **ba**, **bc**, **ca**, **cb**.\n",
    "\n",
    "Complete the function below, so we can count all n-length strings for an alphabet of any length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d5e4abf4ecf43a521ede1f5f84cf811",
     "grade": false,
     "grade_id": "cell-e8239fe099ac2509",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def number_of_permutatuions(number_of_letters, sequence_len):\n",
    "    \"\"\" Counts the number of possible letter permutations in a string.\n",
    "    \n",
    "    This function takes a number of letters in an alphabet and the desired length of a string,\n",
    "    and outputs the number of all possible letter permutations in a string of this length.\n",
    "    A string can contain the same letter \"sequence_len\" times.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    number_of_letters : int \n",
    "        a number of letters in an alphabet\n",
    "    sequence_len : int\n",
    "        the length of a string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    num_of_permutatuions : int\n",
    "        the number of all possible strings of a given length with a given alphabet\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    num_of_permutations=number_of_letters**(sequence_len)\n",
    "    return num_of_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f426decdbf9f229278ed74cabc27718c",
     "grade": true,
     "grade_id": "cell-305f8cc935f8beab",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(number_of_permutatuions(2,2), 4)\n",
    "assert_equal(number_of_permutatuions(2,1), 2)\n",
    "assert_equal(number_of_permutatuions(3,2), 9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "806dc3bb3de4d0c37589245b539b8c97",
     "grade": false,
     "grade_id": "cell-b8c1ea1279ede03e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.5 <a class=\"anchor\" id=\"subtask_1_5\"></a>\n",
    "### Get letter pair counts (3 points)\n",
    "\n",
    "Same as with single letters, some sequences of language units are more probable than others. Not all strings of two letters are actually possible in English. A fact like this can be used in such applications as predictive texting: your phone suggests what might be the next word you need based on previous words you typed (not all word sequences are possible or equally probable).\n",
    "\n",
    "In the following task, you'll need to count all two-letter strings that appeared in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02640eb7b1cf231e3fd8439e772803f5",
     "grade": false,
     "grade_id": "cell-faf866b8fc92dc87",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def count_and_sort_pairs(letters):\n",
    "    i=0\n",
    "    pairs=collections.Counter()\n",
    "    while i<len(letters)-1:\n",
    "        string=letters[i]+letters[i+1]\n",
    "        pairs[string]+=1\n",
    "        i+=1\n",
    "    pairs_sorted=sorted(pairs.items(), key=lambda pair: pair[1], reverse=True)\n",
    "    \n",
    "    \"\"\" This function counts letter pairs and sorts them according to their frequency.\n",
    "    \n",
    "    This function takes a text represented as a list of lowercase letters\n",
    "    and converts it into a sorted list of tuples, where the first element\n",
    "    is a two-letter string, and the second element is the count of this letter pair in the text.\n",
    "    The first element of a list should be a tuple for the most frequent pair.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    letters : list of strings\n",
    "        text as a list of lowercase letters \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pairs_sorted : list of (str, int) tuples\n",
    "        a list of tuples (letter_pair, count) sorted by the count element \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return pairs_sorted\n",
    "\n",
    "bug_pairs_sorted = count_and_sort_pairs(bug_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4f34badcf4661bb1047fefac0458270",
     "grade": true,
     "grade_id": "cell-a02209796380d0cc",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if the function returns a list\n",
    "assert_equal(type(count_and_sort_pairs(bug_letters)), list)\n",
    "\n",
    "# checks if the function returns a list of tuples\n",
    "assert_equal(type(count_and_sort_pairs(bug_letters)[0]), tuple)\n",
    "\n",
    "# checks if the function returns a list of tuples (str, int)\n",
    "assert_equal((type(count_and_sort_pairs(bug_letters)[0][0]),type(count_and_sort_pairs(bug_letters)[0][1])), (str,int))\n",
    "\n",
    "# checks that the most frequent pair was seen 1800 times\n",
    "assert_equal(count_and_sort_pairs(bug_letters)[0][1], 1800)\n",
    "\n",
    "# checks if a functions works right for the dummy example\n",
    "assert_equal(count_and_sort_pairs(\"aaaabbabc\")[:2], [(\"aa\",3),(\"ab\",2)])\n",
    "assert(count_and_sort_pairs(\"aaaabbabc\")[2] in [(\"ba\",1), (\"bb\", 1), (\"bc\", 1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91b7709b87bebe4313b8c7e9d4056d2b",
     "grade": false,
     "grade_id": "cell-dcb81e3883d219e6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.6 <a class=\"anchor\" id=\"subtask_1_6\"></a>\n",
    "### Letter pair frequency analysis (3 points)\n",
    "Using the sorted list you've created, answer the following questions in the cell below:\n",
    "\n",
    "1. How many different two-letter combinations have you actually encountered in the data? (0.6 points)\n",
    "2. What fraction of all theoretically possible two-letter strings is it? (0.6 points)\n",
    "3. What is the most frequent two-letter string in English? (make a mental note if it is surprising or not) (0.6 points)\n",
    "4. What is the probability of seeing a pair where both letters are the same? (0.6 points)\n",
    "5. What is the probability of a pair starting with 'm'? (0.6 points)\n",
    "\n",
    "Type your answers in the cell below. You can create an additional cell to do the calculations if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c945914691bbb4bbddffbe3f48176940",
     "grade": false,
     "grade_id": "cell-f59c76bc63d556f5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n",
      "0.7677514792899408\n",
      "th\n",
      "0.03459875060067275\n",
      "0.025725955927781975\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# put your answer to question 1 as an int into the variable below\n",
    "# For example:\n",
    "# n_pairs = 1000\n",
    "n_pairs = len(bug_pairs_sorted) ##FILL IN THE ANSWER\n",
    "print(n_pairs)\n",
    "\n",
    "# put your answer to question 2 as a float into the variable below\n",
    "# For example:\n",
    "frac_of_pairs = n_pairs/(len(sorted_letters)**2) ##FILL IN THE ANSWER\n",
    "print(frac_of_pairs)\n",
    "# put your answer to question 3 as a string into the variable below\n",
    "# For example:\n",
    "# most_frequent_pair = 'ab'\n",
    "most_frequent_pair = bug_pairs_sorted[0][0] ##FILL IN THE ANSWER\n",
    "print(most_frequent_pair)\n",
    "# put your answer to question 4 as a float into the variable below\n",
    "# For example:\n",
    "p_same_letters = 0\n",
    "for pair in bug_pairs_sorted:\n",
    "    if pair[0][0]==pair[0][1]:\n",
    "        p_same_letters+=float(pair[1])\n",
    "p_same_letters=p_same_letters/len(bug_pai)\n",
    "\n",
    "print(p_same_letters/(len(bug_letters)-1))\n",
    "# put your answer to question 5 as a float into the variable below\n",
    "# For example:\n",
    "# p_starts_with_m = 0.5\n",
    "m_letters = 0\n",
    "for pair in bug_pairs_sorted:\n",
    "    if pair[0][0]=='m':\n",
    "        m_letters+=float(pair[1])\n",
    "p_starts_with_m = m_letters/(len(bug_letters)-1) ##FILL IN THE ANSWER\n",
    "print(p_starts_with_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9bfbbd4c0e936eef40ce255f395405e",
     "grade": true,
     "grade_id": "cell-30230a0028dd5352",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if your answer is an int\n",
    "assert_equal(type(n_pairs) , int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69170691169c268d24aa4714f35a0dff",
     "grade": true,
     "grade_id": "cell-43fa0a46b8247515",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a float\n",
    "assert_equal(type(frac_of_pairs), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9874666bfb7aabc1a637c01188b64675",
     "grade": true,
     "grade_id": "cell-e8ed30f2e25d81d3",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a string\n",
    "assert_equal(type(most_frequent_pair),str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19a35d9b8a2cc60cfd247ef0d3ff1634",
     "grade": true,
     "grade_id": "cell-100f117c7f64f588",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a float\n",
    "assert_equal(type(p_same_letters),float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e64dd0e26aa6835a8c7224137090d4f7",
     "grade": true,
     "grade_id": "cell-ee860b00f1a8983a",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a float\n",
    "assert_equal(type(p_starts_with_m),float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assert_almost_equal(p_same_letters, 0.03459875060067275, 3)\n",
    "p_same_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cd89a2d95d494d55e92bd6f29c57008",
     "grade": false,
     "grade_id": "cell-3bfa916e5c65d448",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 2 <a class=\"anchor\" id=\"task_2\"></a>\n",
    "## Word Tokenization\n",
    "In this task, you will create a function that splits the text into more elaborate units than just letters: words\n",
    "\n",
    "\n",
    "Text data is a part of virtually any NLP application. Sometimes you're lucky, and instead of plain raw text you get nice and clean one, but this is not always the case. Before getting your hands dirty with your actual application, you would most probably need to perform some manipulation to the text. For instance, separate it into words and sentences, remove unwanted symbols. Different tasks require different preprocessing techniques. In this task we'll use some simple ones.\n",
    "\n",
    "It's not trivial to separate words from a string of text. The first thing that needs to be decided is what to count as a word. Should punctuation and numbers be considered words? Should *frogs* and *frog* be considered the same word? What about *Frog*, *frog* and *FROG*? Before answering those questions, let's make sure we are on the same page and discuss some terminology.\n",
    "\n",
    "When talking about words, we can mean several different things: lemmas, word types and word tokens.\n",
    "\n",
    "* **Lemma** - an identifier of a set of lexical forms sharing the same stem (*run* is the lemma for *runs* and *running*), a dictionary form of a word.\n",
    "* **Word type** - a distinct unit in a text (all the instances of *runs* are counted once).\n",
    "* **Word token** - every instance of word occurrence (every instance of *runs* counted as a separate word token).\n",
    "\n",
    "Thus:\n",
    "* **Tokenization** - a process of separating out word tokens from text\n",
    "* **Lemmatization** - a process of assigning a group of word forms their lemma, and further separating out these lemmas from text\n",
    "\n",
    "Generally, English doesn't require lemmatization since it has quite a limited number of word forms. For this reason, we'll leave this task out, for now, and focus on tokenization instead.\n",
    "\n",
    "Let's create a tokenizer that considers numbers and punctuation as tokens and doesn't separate hyphenated words like *dum-dum*. For that you'll need:\n",
    "- regular expressions\n",
    "- string operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a089fd4e5c20509826a1775bbf06883",
     "grade": false,
     "grade_id": "cell-91014079a96c570a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2.1 <a class=\"anchor\" id=\"subtask_2_1\"></a>\n",
    "### Tokenize by whitespaces (1 point)\n",
    "Let's start off by separating words just by whitespaces and see what happens to our dummy sentence example: \n",
    "*It's a dum-dum example, we'll place it here to prove a point. Also look at this number: 300.99.*\n",
    "\n",
    "HINT: One string method is particularly useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63df4a2b5642fe762a17b3157d650605",
     "grade": false,
     "grade_id": "cell-323fdf945301cde7",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's\", 'a', 'dum-dum', 'example,', \"we'll\", 'place', 'it', 'here', 'to', 'prove', 'a', 'point.', 'Also,', 'look', 'at', 'this', 'number:', '300.99.']\n"
     ]
    }
   ],
   "source": [
    "dummy_example = \"It's a dum-dum example, we'll place it here to prove a point. Also, look at this number: 300.99.\"\n",
    "\n",
    "def whitespace_tokenize(raw_string):    \n",
    "    \"\"\"This function tokenizes strings by whitespaces.\n",
    "    \n",
    "    Any whitespace separator should work. \n",
    "    For example, this function should be able to tokenize by '\\n',\n",
    "    and two consecutive whitespaces should be regarded as a single separator.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_string : str\n",
    "        some text to tokenize\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    whitespace_tokenized : list of strings\n",
    "        list of tokens \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    whitespace_tokenized=raw_string.split()\n",
    "    \n",
    "    return whitespace_tokenized\n",
    "\n",
    "dum_dum_example = whitespace_tokenize(dummy_example)\n",
    "# see what you've got\n",
    "print(dum_dum_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f1d24b7aeca9862d0cec387a5a0ae7a",
     "grade": true,
     "grade_id": "cell-f63d5a4956e3323c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if the first token is correct\n",
    "assert_equal(dum_dum_example[0], \"It's\")\n",
    "\n",
    "# checks if number of tokens is correct\n",
    "assert_equal(len(dum_dum_example), 18)\n",
    "\n",
    "# checks if all whitespaces were removed\n",
    "assert_equal(any([' ' in t for t in dum_dum_example]), False)\n",
    "\n",
    "# checks if double whitespaces are removed\n",
    "assert_equal(whitespace_tokenize('  a  a  '), ['a','a'])\n",
    "\n",
    "# checks if tab ia removed too\n",
    "assert_equal(whitespace_tokenize('  a \\t a  '), ['a','a'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65ad19ea848820b8d8e7c093ae0857c0",
     "grade": false,
     "grade_id": "cell-ad4e2d19588ca13a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2.2 <a class=\"anchor\" id=\"subtask_2_2\"></a>\n",
    "### Tokenize with regular expressions (5 points)\n",
    "\n",
    "As can be seen from the dummy example, it's not enough to just separate words by the whitespaces. This way we get tokens like *'example,'*, *'point.'* and *'number:'*. It's not ideal because we would actually like to have punctuation marks as separate tokens but keep them inside the items like prices and numbers (4.99). Thus, we need something more complex: a regular expression.\n",
    "\n",
    "\n",
    "If you've never used regular expressions before or you've never used them in Python, you can read about how they work with the re module [here](https://docs.python.org/3/howto/regex.html). \n",
    "\n",
    "But to give you a simple example, you can think of a regular expression as a shoe that only fits some of the strings. For example, regular expression 'a' (`re.compile(\"a\")`) only fits a string 'a', but regular expression with a special character '\\d' (`re.compile(\"\\d\")`) fits any digit '0','1','2',...'9'. Regular expression 'a|\\d' (`re.compile(\"a|\\d\")`) fits to 'a' string OR to any string with a digit ('|' plays a role of disjunction). You can ask to find all the substrings of a string that match some regular expression pattern with the following piece of code: `re.findall(regular_expression, string_to_look_for_matches)`. More examples are in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fb1151e43d761da56b0009042763b10",
     "grade": false,
     "grade_id": "cell-5cd677d2ecfbef29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '7']\n",
      "['a', 'a']\n",
      "['a1']\n",
      "['a', '1', '2', '3', '7', 'a']\n",
      "['123', '7']\n"
     ]
    }
   ],
   "source": [
    "# example of regular expression usage\n",
    "import re\n",
    "\n",
    "regex_digit = re.compile(\"\\d\") # any digit\n",
    "regex_a = re.compile(\"a\") # only letter 'a'\n",
    "regex_a_digit = re.compile(\"a\\d\") # letter 'a' followed by any digit \n",
    "regex_a_or_digit = re.compile(\"a|\\d\") # letter 'a' OR any digit \n",
    "regex_digit_once_or_more = re.compile(\"\\d+\") # any digit one or more times\n",
    "\n",
    "print(re.findall(regex_digit ,'a123hnd7hjaf'))\n",
    "print(re.findall(regex_a ,'a123hnd7hjaf'))\n",
    "print(re.findall(regex_a_digit ,'a123hnd7hjaf'))\n",
    "print(re.findall(regex_a_or_digit ,'a123hnd7hjaf'))\n",
    "print(re.findall(regex_digit_once_or_more ,'a123hnd7hjaf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8ce7410a4907cc9a10033ef860d0d88",
     "grade": false,
     "grade_id": "cell-2950c20ef363541c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can use regular expressions to describe what type of substring we want to get out of a string. In addition to separation by whitespaces, we want to keep words or numbers with a hyphen, an apostrophe or a point inside, and we want to split punctuation marks from the end of words. For these means, you'll need to write a regular expression that matches:\n",
    "\n",
    "- all alphanumeric strings with hyphen, apostrophe or point inside (i.e should be able to find \"44.44\",\"a-ha\",\"it's\")\n",
    "\n",
    "**OR**\n",
    "- any non-whitespace character followed between zero and unlimited times by any alphanumeric character (this part disregards whitespaces and separates punctuation from the end of words)\n",
    "\n",
    "HINT1: \"\\S\" - non-whitespace character \n",
    "\n",
    "HINT2: \"\\w\" - alphanumeric character\n",
    "\n",
    "HINT3: \"[123]\" - one of the characters in the brackets\n",
    "\n",
    "HINT4: \"*\" - zero or more times\n",
    "\n",
    "HINT5: more hints in this [cheat sheet](https://www.rexegg.com/regex-quickstart.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "174a2e27e1a37168c79394d8dd2c7682",
     "grade": false,
     "grade_id": "cell-3c1fce0956ef8f25",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a dum-dum example, we'll place it here to prove a point. Also, look at this number: 300.99.\n",
      "[\"It's\", 'a', 'dum-dum', 'example', ',', \"we'll\", 'place', 'it', 'here', 'to', 'prove', 'a', 'point', '.', 'Also', ',', 'look', 'at', 'this', 'number', ':', '300.99', '.']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# put a compiled a regular expression tokenizer into a variabe below\n",
    "# For example:\n",
    "# regex_tokenizer = re.compile('\\w+')\n",
    "regex_tokenizer = re.compile('\\w+[-.\\']?\\w+|\\S') ## FILL IN THE ANSWER\n",
    "\n",
    "# look if our dummy example is now tokenized properly\n",
    "print(dummy_example)\n",
    "print(re.findall(regex_tokenizer, dummy_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdf73face8cfe363b03adf8888b31d9e",
     "grade": true,
     "grade_id": "cell-7936f42e9fa94a1e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if punctuation marks are separated from the end of a word\n",
    "assert_equal(re.findall(regex_tokenizer, \"hello!.?\"),['hello', '!', '.', '?'])\n",
    "\n",
    "# checks if punctuation marks are separated from the end of a word and words are whitespace separated\n",
    "assert_equal(re.findall(regex_tokenizer, \"well,  well, well\"),['well', ',', 'well', ',', 'well'])\n",
    "\n",
    "# checks if words and numbers with -'. inside are kept intact\n",
    "assert_equal(re.findall(regex_tokenizer, \"bye-bye, we'll call you at 3.15\"),\n",
    "             ['bye-bye', ',', \"we'll\", 'call', 'you', 'at', '3.15'])\n",
    "\n",
    "# checks if the last token in the dummy sentence is correct\n",
    "assert_equal(re.findall(regex_tokenizer, dummy_example)[-1], '.')\n",
    "\n",
    "# checks if number of tokens in the dummy sentence is correct\n",
    "assert_equal(len(re.findall(regex_tokenizer, dummy_example)), 23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abdb1684e147e8aba44b083f0b35e706",
     "grade": false,
     "grade_id": "cell-0868a2120ad8852f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2.3 <a class=\"anchor\" id=\"subtask_2_3\"></a>\n",
    "### Use Treebank tokenizer (1 points)\n",
    "\n",
    "As you've already noticed, the process of creating a tokenizer is pretty complicated. There are many more things to consider, and a tokenizer should be chosen in accordance with a task. For example, we might also want to capture abbreviations (U.S.A.), percentages (82%) or URLs.\n",
    "\n",
    "Luckily, there are already several good tokenizers implemented for us. For instance, the NLTK package has several (https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize). \n",
    "\n",
    "Let's tokenize our text using the Treebank tokenizer. It uses regular expressions to tokenize text so that tokens match those used in a popular [Penn Treebank](https://web.archive.org/web/19970614160127/http://www.cis.upenn.edu/~treebank/) dataset. This tokenizer also assumes that the text has already been segmented into sentences. Perform sentence segmentation using NLTK's `sent_tokenize()`. Don't forget to lowercase the tokens after tokenizing. If you lowercase before tokenization, it may have an effect on the segmentation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e179e47a7f359bd0e6d1a19a17c765b",
     "grade": false,
     "grade_id": "cell-b1ec48f823b7c5c4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'a', 'dum-dum', 'example', ',', 'we', \"'ll\", 'place', 'it', 'here', 'to', 'prove', 'a', 'point.', 'Also', ',', 'look', 'at', 'this', 'number', ':', '300.99', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jarvint12/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n",
    "\n",
    "# here is how the Treebank tokenizer handles the dummy sentence\n",
    "print(TreebankWordTokenizer().tokenize(dummy_example))\n",
    "\n",
    "def tokenize_and_lowercase(file_name):\n",
    "    with open(file_name, 'r') as fr:\n",
    "        string=fr.read()\n",
    "    \n",
    "    \"\"\"This function tokenizes text files into lowercased tokens with TreebankWordTokenizer\n",
    "    \n",
    "    Read a text file into a string,\n",
    "    tokenize this string into sentences using sent_tokenize(),\n",
    "    tokenize each sentence into tokens using TreebankWordTokenizer().tokenize(),\n",
    "    lowercase each token\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to the text file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tokens : list of strings\n",
    "        text as a list of lowercased tokens\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    sentences=sent_tokenize(string)\n",
    "    tokens=list()\n",
    "    for sentence in sentences:\n",
    "        token_list=TreebankWordTokenizer().tokenize(sentence)\n",
    "        for token in token_list:\n",
    "            tokens.append(token.lower())\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_bug = tokenize_and_lowercase('/coursedata/01-text-processing/the_gold-bug.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "911d0e69f4745c1ca8948b586369006a",
     "grade": true,
     "grade_id": "cell-277001137d76c0ba",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if the number of tokens is correct\n",
    "assert_equal(len(tokenize_and_lowercase('/coursedata/01-text-processing/the_gold-bug.txt')), 16756)\n",
    "# checks if all tokens are lowercased\n",
    "assert_equal(all([x.lower for x in tokenize_and_lowercase('/coursedata/01-text-processing/the_gold-bug.txt')]), True)\n",
    "# checks if the first token is correct\n",
    "assert_equal(tokenize_and_lowercase('/coursedata/01-text-processing/the_gold-bug.txt')[0], 'the')\n",
    "# checks if the last 5 tokens are correct\n",
    "assert_equal(tokenize_and_lowercase('/coursedata/01-text-processing/the_gold-bug.txt')[-5:], ['who', 'shall', 'tell', '?', '”'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5f30f2ab53170b5396a47b5580e957b",
     "grade": false,
     "grade_id": "cell-c5b82c31e7e96a54",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 3 <a class=\"anchor\" id=\"task_3\"></a>\n",
    "## Word frequencies\n",
    "In this task we will explore the distribution of word frequencies and discuss what influence it can have on different NLP tasks.\n",
    "\n",
    "## 3.1 <a class=\"anchor\" id=\"subtask_3_1\"></a>\n",
    "### Analyse word frequencies (3 points)\n",
    "You've already recorded the statistics of letters and letter pairs, now you can repurpose those functions to answer the following questions:\n",
    "\n",
    "1. How many word tokens are there in the text? (0.6 points)\n",
    "2. How many word types are there in the text? (0.6 points)\n",
    "3. What are 10 most frequent words? Report them as a list starting with the most frequent one? (0.6 points)\n",
    "4. What is the fraction of word types (out of all word types) that appeared in the text only 2 times or less? (0.6 points)\n",
    "5. What is the fraction of word types (out of all word types) that appeared in the text 50 times or more? (0.6 points)\n",
    "\n",
    "Type your answers in the cell below. You can create an additional cell to do the calculations if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "966eacffdb573dac4221329a1370d3f2",
     "grade": false,
     "grade_id": "cell-a8f655518f58b3fd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16756\n",
      "2845\n",
      "[',', 'the', '.', 'of', 'and', 'i', 'a', 'to', '“', 'in']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# put your answer to question 1 as an int into the variable below\n",
    "# For example:\n",
    "# n_word_tokens = 1000\n",
    "n_word_tokens = len(tokenized_bug) ##FILL IN THE ANSWER\n",
    "print(n_word_tokens)\n",
    "# put your answer to question 3 as an int into the variable below\n",
    "# For example:\n",
    "# n_word_types = 1000\n",
    "n_word_types = len(set(tokenized_bug)) ##FILL IN THE ANSWER remove duplicates, \n",
    "print(n_word_types)\n",
    "\n",
    "# put your answer to question 3 as list of strings into the variable below\n",
    "# For example:\n",
    "# top_ten_words = ['hello', 'world']\n",
    "word_counts=collections.Counter()\n",
    "for word in tokenized_bug:\n",
    "    word_counts[word]+=1\n",
    "sorted_list=sorted(word_counts.items(), key=lambda pair: pair[1], reverse=True)\n",
    "top_ten_words=list()\n",
    "i=0\n",
    "while(i<10):\n",
    "    top_ten_words.append(sorted_list[i][0])\n",
    "    i+=1\n",
    "    ##FILL IN THE ANSWER\n",
    "print(top_ten_words)\n",
    "\n",
    "# put your answer to question 4 as a float into the variable below\n",
    "# For example:\n",
    "sum_rare=0\n",
    "sum_all=0\n",
    "for pair in sorted_list:\n",
    "    if pair[1]<=2:\n",
    "        sum_rare+=1\n",
    "    sum_all+=1\n",
    "frac_2_or_less = sum_rare/sum_all ##FILL IN THE ANSWER\n",
    "\n",
    "# put your answer to question 5 as a float into the variable below\n",
    "# For example:\n",
    "sum_popular=0\n",
    "for pair in sorted_list:\n",
    "    if pair[1]>=50:\n",
    "        sum_popular+=1\n",
    "##FILL IN THE ANSWER\n",
    "frac_50_or_more = sum_popular/sum_all ##FILL IN THE ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f062ec7e940a09f598e84d5c38dc5e8c",
     "grade": true,
     "grade_id": "cell-727ab27821e90861",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if your answer is an int\n",
    "assert_equal(type(n_word_tokens),int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c3cdbbac4acf48b6eab871a7527b12a",
     "grade": true,
     "grade_id": "cell-f40a9b7de3dfe7cb",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if your answer is an int\n",
    "assert_equal(type(n_word_types),int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4374030f37a95c4ce5441f98a7b0df0d",
     "grade": true,
     "grade_id": "cell-e2c84eb3451e0d60",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if your answer is of the right formt\n",
    "assert_equal(type(top_ten_words),list)\n",
    "assert_equal(type(top_ten_words[0]),str)\n",
    "assert_equal(len(top_ten_words),10)\n",
    "# checks if you've got the most frequent token right\n",
    "assert_equal(top_ten_words[0],',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "671063d31b65e77c4d0931e1560a4230",
     "grade": true,
     "grade_id": "cell-9d91ee0fe145007a",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a float\n",
    "assert_equal(type(frac_2_or_less),float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "168fd92376e9b2ab651963c44b87399d",
     "grade": true,
     "grade_id": "cell-b8de5c57ecf06fd4",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a float\n",
    "assert_equal(type(frac_50_or_more),float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "14770f1c92d270f876b6c37337e0a93b",
     "grade": false,
     "grade_id": "cell-fb0a4f7abc9b2f63",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 3.2 <a class=\"anchor\" id=\"subtask_3_2\"></a>\n",
    "### Remove stop words (1 point)\n",
    "As you can see, the most frequent wrod types are not specific to the Poe's story, but are pretty much the same across English language.  \n",
    "\n",
    "In information theory, the more likely an event to occur, the less information it contains. Thus, if an event is not a surprise, it's simply \"old news\". For some natural language applications, it means that words like *to* and *the* don't tell anything important about a text. They are not helpful in recognising its topic or its author, for instance.\n",
    "\n",
    "Such frequent uninformative words are called **stop words**, and, in some cases, they can simply be cleaned out from data. There exist prepared lists of such words in English. Let's remove the ones provided by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f1f38e8d76a434fe35ac5e0f3e5e555",
     "grade": false,
     "grade_id": "cell-fb4b2ec2f039e056",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jarvint12/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words_english = stopwords.words('english')\n",
    "#print(stop_words_english)\n",
    "\n",
    "def remove_stop_words(tokenized_text, stop_words):\n",
    "    \"\"\"This function removes stop words from lowercased tokenized text\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenized_text : list of strings\n",
    "        lowercased text tokens \n",
    "    stop_words : list of strings\n",
    "        a list of words to remove\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    clean_text : list of strings\n",
    "        list of text tokens with stop words removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #print(tokenized_text)\n",
    "    clean_text=list()\n",
    "    for token in tokenized_text:\n",
    "        if not token in stop_words:\n",
    "            clean_text.append(token)\n",
    "    \n",
    "    return clean_text\n",
    "    \n",
    "clean_bug = remove_stop_words(tokenized_bug, stop_words_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b637ed495805f791a5f33d2a8410b0ac",
     "grade": true,
     "grade_id": "cell-09346ef37946973d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if the number of tokens is right\n",
    "assert_equal(len(clean_bug), 9630)\n",
    "\n",
    "# checks if the first token is right\n",
    "assert_equal(clean_bug[0], 'gold-bug')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf2d56227b449fc219cf356e27337f17",
     "grade": false,
     "grade_id": "cell-5a9d38e29ebc7dbe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3.3 <a class=\"anchor\" id=\"subtask_3_3\"></a>\n",
    "### How much is  left without stop words? (1 point)\n",
    "Great! Now you have a clean text that can further be used for such tasks as sentiment analysis, for example. \n",
    "\n",
    "The last thing. What was the fraction of stop word tokens in the tokenized text? Write your answer in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebaf0db0345ae77921d447f099953eda",
     "grade": false,
     "grade_id": "cell-9d0bf9d7bb8f3c8d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4252804965385534\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# put your answer as a float into the variable below\n",
    "# For example:\n",
    "# fraction_of_stop_word_tokens = 0.5\n",
    "fraction_of_stop_word_tokens = (len(tokenized_bug)-len(clean_bug))/len(tokenized_bug) ## FILL IN THE ANSWER\n",
    "print(fraction_of_stop_word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5acf75af84d37d15413a88c750f03f4f",
     "grade": true,
     "grade_id": "cell-6bd0960480895cd5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# checks if your answer is float\n",
    "assert_equal(type(fraction_of_stop_word_tokens),float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38450075ddb05d6a4f51bca2e1ebf62f",
     "grade": false,
     "grade_id": "cell-0e07aeb2833401f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checklist before submission <a class=\"anchor\" id=\"checklist\"></a>\n",
    "### 1\n",
    "To make sure that you didn't forget to import some package or to name some variable, press **Kernel -> Restart** and then **Cell -> Run All**. This way your code will be run exactly in the same order as during the autograding.\n",
    "### 2\n",
    "Click the **Validate** button in the upper menu to check that you haven't missed anything.\n",
    "### 3\n",
    "To submit the notebook, click on the **jupyterhub** logo in the upper left part of the window, choose the **Assignments** folder, and press **submit**. You can submit multiple times, only the last one counts.\n",
    "### 4\n",
    "Please fill in the feedback form in the [Assignment](https://mycourses.aalto.fi/mod/questionnaire/view.php?id=689919) section of Mycoures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
