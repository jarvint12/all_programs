{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_separate_bodyparts.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lwPReLmjsVcN",
        "jTF2ejjUagt1",
        "EAfq18kFetYz"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3BV-tFCh5L8"
      },
      "source": [
        "Aja vain Import ja main-solut"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZF-yIbO90iv"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "A0WKfVqTamrK",
        "outputId": "819770f7-9f0e-4098-e112-672f50aa3ce0"
      },
      "source": [
        "import os\n",
        "import time\n",
        "from subprocess import getoutput\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, cohen_kappa_score, f1_score\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.callbacks import (ModelCheckpoint, TensorBoard)\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.densenet import preprocess_input, DenseNet169\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Ignore python warnings\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "getoutput(\"git clone -l -s https://github.com/jarvint12/ai_project.git ready_dense168\")\n",
        "#https://github.com/bigrewal/Musculoskeletal-Radiographs-abnormality-detection.git\n",
        "#os.chdir('cloned-repo')\n",
        "#from mura_model.src.model.dense169 import get_dense169"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Cloning into 'ready_dense168'...\\nwarning: --local is ignored\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwPReLmjsVcN"
      },
      "source": [
        "# Dense169 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwa67SmlsYfC"
      },
      "source": [
        "def get_dense169(input_shape, learning_rate):\n",
        "    # create the base pre-trained model\n",
        "    dense_169_model = DenseNet169(include_top=False, weights='imagenet', input_shape=(input_shape, input_shape, 3))\n",
        "    x = dense_169_model.output\n",
        "    model = keras.Sequential([dense_169_model, Flatten(), Dense(1, activation='sigmoid')])\n",
        "\n",
        "    for layer in dense_169_model.layers:\n",
        "        layer.trainable = True\n",
        "    adam = optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(optimizer=adam,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTF2ejjUagt1"
      },
      "source": [
        "# Averaged probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q74G93Vyajr7"
      },
      "source": [
        "\n",
        "'''\n",
        "   This method is used to average the probabilties\n",
        "   of all the images in a study.\n",
        "   \n",
        "   @:param probs. Predictions of the neural network\n",
        "   @:param image_paths. file paths of all the images predicted\n",
        "   @:param output file. File to be created which will contain the predictions per study\n",
        "'''\n",
        "def average_probabilities(probs, image_paths, output_file):\n",
        "    averaged_probs = __average(probs, image_paths)\n",
        "    \n",
        "    # Create CSV filE \n",
        "    df = pd.DataFrame(list(averaged_probs.keys()), columns=['study'])\n",
        "    df['label'] = list(averaged_probs.values())\n",
        "    \n",
        "    df.to_csv(output_file, index=False, header=None)\n",
        "    print(output_file + \" Created!\")\n",
        "    return averaged_probs #Addasin, ei ollut return valuea\n",
        "\n",
        "\n",
        "def __average(probs, image_paths):\n",
        "    img_paths = image_paths\n",
        "    predictions = probs\n",
        "\n",
        "    averaged_probabilities = {}\n",
        "\n",
        "    for path in img_paths:\n",
        "        study_name = '/'.join(path.split('/')[0:-1]) + \"/\"\n",
        "        if study_name in averaged_probabilities:\n",
        "            continue\n",
        "\n",
        "        indices = [i for i, s in enumerate(img_paths) if study_name in s]\n",
        "        probs = [predictions[i] for i in indices]\n",
        "\n",
        "        # Based on the current directory structure, Keras has assigned the value 0 (Zero) to abnormal\n",
        "        # and 1 (One) to Normal so in order to make sure that the class indices are in the right order\n",
        "        # we have to subtract the predictions from 1.\n",
        "        average = 1 - int(np.round(np.mean(probs)))\n",
        "\n",
        "        averaged_probabilities[study_name] = average\n",
        "\n",
        "    return averaged_probabilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAfq18kFetYz"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kmbBd2PewF3"
      },
      "source": [
        "def print_all_metrics(y_true, y_pred):\n",
        "    print(\"roc_auc_score: \", roc_auc_score(y_true, y_pred))\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "\n",
        "    print(\"Sensitivity: \", get_sensitivity(tp, fn))\n",
        "    print(\"Specificity: \", get_specificity(tn, fp))\n",
        "    print(\"Cohen-Cappa-Score: \", cohen_kappa_score(y_true, y_pred))\n",
        "    print(\"F1 Score: \", f1_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "def get_sensitivity(tp, fn):\n",
        "    return tp / (tp + fn)\n",
        "\n",
        "\n",
        "def get_specificity(tn, fp):\n",
        "    return tn / (tn + fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlhxWPnY3W32"
      },
      "source": [
        "# main-train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wamg1p691xcO"
      },
      "source": [
        "**Create the Architectures**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nroa_Kel1408"
      },
      "source": [
        "def get_best_model(body_part, model_location):\n",
        "  best_model=None\n",
        "  for root, dirs, files in os.walk(model_location, topdown=True):\n",
        "    for file in files:\n",
        "      if body_part in file:\n",
        "        best_model=root+'/'+file #Supposes, that models are organised from worse to best\n",
        "  if best_model==None:\n",
        "    raise Exception(\"No model found for \"+body_part+\" from directory \"+model_location+'.')\n",
        "  return best_model\n",
        "\n",
        "\n",
        "def main_train(body_part):\n",
        "  input_shape = 320\n",
        "  batch_size = 8\n",
        "  epochs = 10\n",
        "  learning_rate = 0.0001\n",
        "\n",
        "  dense169_mura_single = get_dense169(input_shape, learning_rate)\n",
        "\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rotation_range=30,\n",
        "      horizontal_flip=True,\n",
        "      preprocessing_function=preprocess_input,\n",
        "      validation_split=0)\n",
        "\n",
        "  #print('/content/drive/MyDrive/MURA-v1.2/temp_'+body_part+'/train_data')\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "      '/content/drive/MyDrive/MURA-v1.2/temp_'+body_part+'/train_data',\n",
        "      target_size=(input_shape, input_shape),\n",
        "      batch_size=batch_size,\n",
        "      class_mode='binary',\n",
        "      subset='training')\n",
        "  training_data_size = len(train_generator.filenames)\n",
        "  print(\"Number of Training examples: \", training_data_size)\n",
        "\n",
        "  valid_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    validation_split=0.7)\n",
        "\n",
        "  valid_generator = valid_datagen.flow_from_directory(\n",
        "      '/content/drive/MyDrive/MURA-v1.2/temp_'+body_part+'/valid_data',\n",
        "      target_size=(input_shape, input_shape),\n",
        "      batch_size=batch_size,\n",
        "      class_mode='binary',\n",
        "      shuffle=False,\n",
        "      subset=\"training\")\n",
        "  validation_data_size = len(valid_generator.filenames)\n",
        "\n",
        "  print(\"Number of Validation examples: \", validation_data_size)\n",
        "\n",
        "\n",
        "  test_generator = valid_datagen.flow_from_directory(\n",
        "      '/content/drive/MyDrive/MURA-v1.2/temp_'+body_part+'/valid_data',\n",
        "      target_size=(input_shape, input_shape),\n",
        "      batch_size=batch_size,\n",
        "      class_mode='binary',\n",
        "      shuffle=False,\n",
        "      subset=\"validation\")\n",
        "  \n",
        "  weights = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes)\n",
        "  print(\"Weights: \",weights)\n",
        "\n",
        "  #val_acc muutettava val_accuracyksi\n",
        "  filepath=\"/content/drive/MyDrive/MURA-v1.2/output_separate/dense169-\"\\\n",
        "  +body_part+\"-{epoch:02d}-{val_accuracy:.2f}.hdf5\"  # Change the name based on which model is being trained (epoch number, validation loss)\n",
        "  \n",
        "  model_location=\"/content/drive/MyDrive/MURA-v1.2/output_separate/\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "  tensorboard = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=False)\n",
        "\n",
        "  callbacks_list = [checkpoint, tensorboard]\n",
        "\n",
        "  print(dense169_mura_single.summary())\n",
        "\n",
        "  dense169_mura_single.fit(train_generator,\n",
        "                        validation_data=valid_generator,\n",
        "                        steps_per_epoch=training_data_size // batch_size,\n",
        "                        class_weight={0:weights[0], 1:weights[1]},\n",
        "                        callbacks=callbacks_list, \n",
        "                        validation_steps=validation_data_size // batch_size,\n",
        "                        epochs=epochs)\n",
        "  return get_best_model(body_part, model_location), test_generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpODWdxEhHKA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZmFPTRB3jhs"
      },
      "source": [
        "# Submission mura"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZLH_Vyj3rsx"
      },
      "source": [
        "def submission_mura(body_part, model, test_generator):\n",
        "\n",
        "  model_mura = load_model(model)\n",
        "  N=len(test_generator.filenames)\n",
        "\n",
        "  print(\"Images to be prediced: \", N)\n",
        "\n",
        "  print(\"Predicting ...\")\n",
        "  probs = model_mura.predict_generator(test_generator, steps=N)\n",
        "  print(\"Done!\")\n",
        "\n",
        "  print(probs[:10])\n",
        "  print(test_generator.filenames[:10])\n",
        "\n",
        "  print(len(probs))\n",
        "\n",
        "  file_names = test_generator.filenames\n",
        "  print(file_names[0])\n",
        "  #file_names = [prefix + file for file in file_names]\n",
        "\n",
        "  #print(file_names[0])\n",
        "\n",
        "  #from src.data.postprocessor import average_probabilities\n",
        "\n",
        "  predictions = average_probabilities(probs, file_names, \\\n",
        "  \"/content/drive/MyDrive/MURA-v1.2/output_separate/predictions_\"+body_part+\".csv\")\n",
        "\n",
        "  len(predictions)\n",
        "  return predictions\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn8J5OKh4cry"
      },
      "source": [
        "# Evaluate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8uT7bWq4r2j"
      },
      "source": [
        "**Load trained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbeALg19LqJO"
      },
      "source": [
        "# ABNORMAL_XR_ELBOW_patient11186_study1_positive_image1.png\n",
        "\n",
        "def extract_studies(predictions, filenames, body_part, y_true):\n",
        "  pred = []\n",
        "  true = []\n",
        "\n",
        "  for index, fileName in enumerate(filenames):\n",
        "    if body_part in fileName:\n",
        "        pred.append(predictions[index])\n",
        "        true.append(y_true[index])\n",
        "        \n",
        "\n",
        "  print(\"===== \"+body_part+\" ======\")\n",
        "  print_all_metrics(true,pred)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuXDZP5_4icX"
      },
      "source": [
        "def model_evaluation(model, test_generator, predictions, body_part):\n",
        "  print(\"In model evaluation.\")\n",
        "  print( \"Loading Model\" )\n",
        "  model_mura = load_model(model)\n",
        "  print(\"Model Loaded!\")\n",
        "\n",
        "  #TEST_DIR = \"/content/drive/MyDrive/MURA-v1.2/valid_data/\"\n",
        "  #input_shape = 320\n",
        "  #batch_size = 1\n",
        "\n",
        "  print(test_generator.class_indices)\n",
        "  print(\"Predicting \"+str(len(test_generator.filenames))+\" files.\")\n",
        "  start = time.time()\n",
        "\n",
        "  predictions = model_mura.predict_generator(test_generator, steps=len(test_generator.filenames)/1) #steps=3197//1\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"It took: \", end - start)\n",
        "\n",
        "  predictions = predictions.flatten()\n",
        "  y_true = test_generator.classes\n",
        "  print(\"pred, shape\",predictions.shape)\n",
        "  predictions = np.round(predictions)\n",
        "\n",
        "  prefix=\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/valid_data/\"\n",
        "  file_names = [prefix + file for file in test_generator.filenames]\n",
        "  print(file_names[0])\n",
        "  print(\"lens\",len(file_names), len(predictions), len(y_true))\n",
        "  extract_studies(predictions, file_names, body_part, y_true)\n",
        "\n",
        "  confusion_matrix(y_true, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37xLDEFIPbFG"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_4tNdc8AKp3"
      },
      "source": [
        "Define create directories and move files functions, as well as delete previous models if necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78Zp94ERACJO"
      },
      "source": [
        "def move_files_temp(body_part):\n",
        "  for root, dirs, files in os.walk('/content/drive/MyDrive/MURA-v1.2', topdown=True):\n",
        "    if \"temp_\"+body_part in root:\n",
        "      continue\n",
        "    for file in files:\n",
        "      if body_part in file:\n",
        "        if \"valid_data\" in root:\n",
        "          if \"abnormal\" in root:\n",
        "            os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/valid_data/abnormal/\"+file)\n",
        "          elif \"normal\" in root:\n",
        "            os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/valid_data/normal/\"+file)\n",
        "        elif \"train_data\" in root:\n",
        "         # print(root)\n",
        "          if \"abnormal\" in root:\n",
        "            #print(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/train_data/abnormal/\"+file)\n",
        "            os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/train_data/abnormal/\"+file)\n",
        "          elif \"normal\" in root:\n",
        "            #print(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/train_data/normal/\"+file)\n",
        "            os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/train_data/normal/\"+file)\n",
        "  print(\"All \"+body_part+\" files moved to temporal location.\")\n",
        "\n",
        "def move_files_back(body_part):\n",
        "  for root, dirs, files in os.walk(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part, topdown=True):\n",
        "    for file in files:\n",
        "      if body_part in file:\n",
        "        if \"valid_data\" in root:\n",
        "          if \"abnormal\" in root:\n",
        "            #print(\"abnormal\")\n",
        "            os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/valid_data/abnormal/\"+file)\n",
        "          elif \"normal\" in root:\n",
        "            #print(\"normal\")\n",
        "            os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/valid_data/normal/\"+file)\n",
        "        elif \"train_data\" in root:\n",
        "          #print(root)\n",
        "          #continue\n",
        "          if \"abnormal\" in root:\n",
        "            #print(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/train_data/abnormal/\"+file)\n",
        "            os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/train_data/abnormal/\"+file)\n",
        "          elif \"normal\" in root:\n",
        "            #print(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/train_data/normal/\"+file)\n",
        "            os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/train_data/normal/\"+file)\n",
        "  print(\"All \"+body_part+\" files moved back to their original location.\")\n",
        "\n",
        "\n",
        "def create_directories(body_part):\n",
        "  os.mkdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part)\n",
        "  os.mkdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/train_data\")\n",
        "  os.mkdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/valid_data\")\n",
        "  os.mkdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/train_data/abnormal\")\n",
        "  os.mkdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/train_data/normal\")\n",
        "  os.mkdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/valid_data/abnormal\")\n",
        "  os.mkdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/valid_data/normal\")\n",
        "  print(\"Temporal directories created for program.\")\n",
        "\n",
        "def remove_directories(body_part):\n",
        "  os.rmdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/train_data/abnormal\")\n",
        "  os.rmdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/train_data/normal\")\n",
        "  os.rmdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/valid_data/abnormal\")\n",
        "  os.rmdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/valid_data/normal\")\n",
        "  os.rmdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/train_data\")\n",
        "  os.rmdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/valid_data\")\n",
        "  os.rmdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part)\n",
        "  print(\"All temporal directories removed.\")\n",
        "\n",
        "def delete_old_models(location, body_part):\n",
        "  for root, dirs, files in os.walk(location, topdown=True):\n",
        "    for file in files:\n",
        "      if body_part in file:\n",
        "        os.remove(root+'/'+file)\n",
        "        print(\"Deleted old file, \"+root+'/'+file+'.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT4zV9r7AXvi"
      },
      "source": [
        "Main program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38qrvXtfPagt",
        "outputId": "ca41e6b7-aa73-4476-dc2d-8b0bf3590d4a"
      },
      "source": [
        "\n",
        "\n",
        "def main():\n",
        "  #body_part=\"HUMERUS\" \n",
        "  body_parts=[\"SHOULDER\", \"WRIST\"]\n",
        "  for part in body_parts:\n",
        "    body_part=part\n",
        "    delete_old_models(\"/content/drive/MyDrive/MURA-v1.2/output_separate/\", body_part)\n",
        "    print(\"Handling body part \"+body_part+'.')\n",
        "    if not os.path.isdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part):\n",
        "      create_directories(body_part)\n",
        "    move_files_temp(body_part)\n",
        "    time.sleep(5)\n",
        "    if not os.path.isdir(\"/content/drive/MyDrive/MURA-v1.2/output_separate\"):\n",
        "      os.mkdir(\"/content/drive/MyDrive/MURA-v1.2/output_separate\")\n",
        "    model, test_generator=main_train(body_part)\n",
        "    print(\"The best model: \"+model)\n",
        "    predictions=submission_mura(body_part, model, test_generator)\n",
        "    model_evaluation(model, test_generator, predictions, body_part)\n",
        "    move_files_back(body_part)\n",
        "    time.sleep(2)\n",
        "    remove_directories(body_part)\n",
        "\n",
        "\n",
        "main()\n",
        "#Next: directory structure with 14 folders (wrist_normal, wrist_abnormal)\n",
        "#flow from directory class_mode=\"categorical\"\n",
        "#Sitten malli toimii kuin normaalistikin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deleted old file, /content/drive/MyDrive/MURA-v1.2/output_separate//dense169-SHOULDER-01-0.59.hdf5.\n",
            "Deleted old file, /content/drive/MyDrive/MURA-v1.2/output_separate//dense169-SHOULDER-03-0.60.hdf5.\n",
            "Deleted old file, /content/drive/MyDrive/MURA-v1.2/output_separate//dense169-SHOULDER-04-0.61.hdf5.\n",
            "Deleted old file, /content/drive/MyDrive/MURA-v1.2/output_separate//dense169-SHOULDER-05-0.65.hdf5.\n",
            "Handling body part SHOULDER.\n",
            "All SHOULDER files moved to temporal location.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "51879936/51877672 [==============================] - 0s 0us/step\n",
            "Found 8379 images belonging to 2 classes.\n",
            "Number of Training examples:  8379\n",
            "Found 170 images belonging to 2 classes.\n",
            "Number of Validation examples:  170\n",
            "Found 393 images belonging to 2 classes.\n",
            "Weights:  [1.00515835 0.99489432]\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "densenet169 (Functional)     (None, 10, 10, 1664)      12642880  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 166400)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 166401    \n",
            "=================================================================\n",
            "Total params: 12,809,281\n",
            "Trainable params: 12,650,881\n",
            "Non-trainable params: 158,400\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "   1/1047 [..............................] - ETA: 0s - loss: 0.9103 - accuracy: 0.6250WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "   2/1047 [..............................] - ETA: 5:22 - loss: 1.1019 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1408s vs `on_train_batch_end` time: 0.4806s). Check your callbacks.\n",
            "1047/1047 [==============================] - ETA: 0s - loss: 0.9429 - accuracy: 0.6402\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.77381, saving model to /content/drive/MyDrive/MURA-v1.2/output_separate/dense169-SHOULDER-01-0.77.hdf5\n",
            "1047/1047 [==============================] - 2376s 2s/step - loss: 0.9429 - accuracy: 0.6402 - val_loss: 0.4716 - val_accuracy: 0.7738\n",
            "Epoch 2/10\n",
            "1047/1047 [==============================] - ETA: 0s - loss: 0.6659 - accuracy: 0.7047\n",
            "Epoch 00002: val_accuracy did not improve from 0.77381\n",
            "1047/1047 [==============================] - 303s 289ms/step - loss: 0.6659 - accuracy: 0.7047 - val_loss: 0.7588 - val_accuracy: 0.6845\n",
            "Epoch 3/10\n",
            "1047/1047 [==============================] - ETA: 0s - loss: 0.6567 - accuracy: 0.7076\n",
            "Epoch 00003: val_accuracy did not improve from 0.77381\n",
            "1047/1047 [==============================] - 302s 289ms/step - loss: 0.6567 - accuracy: 0.7076 - val_loss: 0.5303 - val_accuracy: 0.7083\n",
            "Epoch 4/10\n",
            "1047/1047 [==============================] - ETA: 0s - loss: 0.6651 - accuracy: 0.7135\n",
            "Epoch 00004: val_accuracy did not improve from 0.77381\n",
            "1047/1047 [==============================] - 303s 289ms/step - loss: 0.6651 - accuracy: 0.7135 - val_loss: 0.6998 - val_accuracy: 0.7321\n",
            "Epoch 5/10\n",
            "1047/1047 [==============================] - ETA: 0s - loss: 0.6144 - accuracy: 0.7272\n",
            "Epoch 00005: val_accuracy did not improve from 0.77381\n",
            "1047/1047 [==============================] - 302s 289ms/step - loss: 0.6144 - accuracy: 0.7272 - val_loss: 0.5231 - val_accuracy: 0.7560\n",
            "Epoch 6/10\n",
            "1047/1047 [==============================] - ETA: 0s - loss: 0.5748 - accuracy: 0.7460\n",
            "Epoch 00006: val_accuracy improved from 0.77381 to 0.82143, saving model to /content/drive/MyDrive/MURA-v1.2/output_separate/dense169-SHOULDER-06-0.82.hdf5\n",
            "1047/1047 [==============================] - 303s 289ms/step - loss: 0.5748 - accuracy: 0.7460 - val_loss: 0.5237 - val_accuracy: 0.8214\n",
            "Epoch 7/10\n",
            "1047/1047 [==============================] - ETA: 0s - loss: 0.5863 - accuracy: 0.7396\n",
            "Epoch 00007: val_accuracy did not improve from 0.82143\n",
            "1047/1047 [==============================] - 300s 286ms/step - loss: 0.5863 - accuracy: 0.7396 - val_loss: 0.5197 - val_accuracy: 0.7560\n",
            "Epoch 8/10\n",
            "1047/1047 [==============================] - ETA: 0s - loss: 0.5602 - accuracy: 0.7519\n",
            "Epoch 00008: val_accuracy did not improve from 0.82143\n",
            "1047/1047 [==============================] - 296s 283ms/step - loss: 0.5602 - accuracy: 0.7519 - val_loss: 0.5613 - val_accuracy: 0.7500\n",
            "Epoch 9/10\n",
            "1047/1047 [==============================] - ETA: 0s - loss: 0.5770 - accuracy: 0.7442\n",
            "Epoch 00009: val_accuracy did not improve from 0.82143\n",
            "1047/1047 [==============================] - 293s 280ms/step - loss: 0.5770 - accuracy: 0.7442 - val_loss: 0.5793 - val_accuracy: 0.7679\n",
            "Epoch 10/10\n",
            "1047/1047 [==============================] - ETA: 0s - loss: 0.5190 - accuracy: 0.7671\n",
            "Epoch 00010: val_accuracy did not improve from 0.82143\n",
            "1047/1047 [==============================] - 291s 278ms/step - loss: 0.5190 - accuracy: 0.7671 - val_loss: 0.5365 - val_accuracy: 0.7679\n",
            "The best model: /content/drive/MyDrive/MURA-v1.2/output_separate//dense169-SHOULDER-06-0.82.hdf5\n",
            "Images to be prediced:  393\n",
            "Predicting ...\n",
            "WARNING:tensorflow:From <ipython-input-6-e8cc55bcc338>:9: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.predict, which supports generators.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 393 batches). You may need to use the repeat() function when building your dataset.\n",
            "Done!\n",
            "[[0.13819689]\n",
            " [0.02377414]\n",
            " [0.53992665]\n",
            " [0.1473461 ]\n",
            " [0.03065137]\n",
            " [0.80842435]\n",
            " [0.6434523 ]\n",
            " [0.40212646]\n",
            " [0.4630371 ]\n",
            " [0.15347187]]\n",
            "['abnormal/XR_SHOULDER_patient11186_study1_positive_image1.png', 'abnormal/XR_SHOULDER_patient11186_study1_positive_image2.png', 'abnormal/XR_SHOULDER_patient11186_study1_positive_image3.png', 'abnormal/XR_SHOULDER_patient11188_study1_positive_image1.png', 'abnormal/XR_SHOULDER_patient11188_study1_positive_image2.png', 'abnormal/XR_SHOULDER_patient11188_study1_positive_image3.png', 'abnormal/XR_SHOULDER_patient11188_study2_positive_image1.png', 'abnormal/XR_SHOULDER_patient11188_study2_positive_image2.png', 'abnormal/XR_SHOULDER_patient11188_study2_positive_image3.png', 'abnormal/XR_SHOULDER_patient11195_study1_positive_image1.png']\n",
            "393\n",
            "abnormal/XR_SHOULDER_patient11186_study1_positive_image1.png\n",
            "/content/drive/MyDrive/MURA-v1.2/output_separate/predictions_SHOULDER.csv Created!\n",
            "In model evaluation.\n",
            "Loading Model\n",
            "Model Loaded!\n",
            "{'abnormal': 0, 'normal': 1}\n",
            "Predicting 393 files.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 393.0 batches). You may need to use the repeat() function when building your dataset.\n",
            "It took:  4.926657438278198\n",
            "pred, shape (393,)\n",
            "/content/drive/MyDrive/MURA-v1.2/temp_SHOULDER/valid_data/abnormal/XR_SHOULDER_patient11186_study1_positive_image1.png\n",
            "lens 393 393 393\n",
            "===== SHOULDER ======\n",
            "roc_auc_score:  0.7612288245350464\n",
            "Sensitivity:  0.7286432160804021\n",
            "Specificity:  0.7938144329896907\n",
            "Cohen-Cappa-Score:  0.5219844206930462\n",
            "F1 Score:  0.7552083333333335\n",
            "All SHOULDER files moved back to their original location.\n",
            "All temporal directories removed.\n",
            "Handling body part WRIST.\n",
            "Temporal directories created for program.\n",
            "All WRIST files moved to temporal location.\n",
            "Found 9752 images belonging to 2 classes.\n",
            "Number of Training examples:  9752\n",
            "Found 199 images belonging to 2 classes.\n",
            "Number of Validation examples:  199\n",
            "Found 460 images belonging to 2 classes.\n",
            "Weights:  [1.22297467 0.84579358]\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "densenet169 (Functional)     (None, 10, 10, 1664)      12642880  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 166400)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 166401    \n",
            "=================================================================\n",
            "Total params: 12,809,281\n",
            "Trainable params: 12,650,881\n",
            "Non-trainable params: 158,400\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 0.9475 - accuracy: 0.6945\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.63021, saving model to /content/drive/MyDrive/MURA-v1.2/output_separate/dense169-WRIST-01-0.63.hdf5\n",
            "1219/1219 [==============================] - 2670s 2s/step - loss: 0.9475 - accuracy: 0.6945 - val_loss: 0.7826 - val_accuracy: 0.6302\n",
            "Epoch 2/10\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 0.8309 - accuracy: 0.7193\n",
            "Epoch 00002: val_accuracy improved from 0.63021 to 0.66667, saving model to /content/drive/MyDrive/MURA-v1.2/output_separate/dense169-WRIST-02-0.67.hdf5\n",
            "1219/1219 [==============================] - 362s 297ms/step - loss: 0.8309 - accuracy: 0.7193 - val_loss: 1.0402 - val_accuracy: 0.6667\n",
            "Epoch 3/10\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 0.7351 - accuracy: 0.7443\n",
            "Epoch 00003: val_accuracy improved from 0.66667 to 0.75521, saving model to /content/drive/MyDrive/MURA-v1.2/output_separate/dense169-WRIST-03-0.76.hdf5\n",
            "1219/1219 [==============================] - 355s 292ms/step - loss: 0.7351 - accuracy: 0.7443 - val_loss: 1.5915 - val_accuracy: 0.7552\n",
            "Epoch 4/10\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 0.7649 - accuracy: 0.7352\n",
            "Epoch 00004: val_accuracy did not improve from 0.75521\n",
            "1219/1219 [==============================] - 350s 287ms/step - loss: 0.7649 - accuracy: 0.7352 - val_loss: 0.7617 - val_accuracy: 0.7135\n",
            "Epoch 5/10\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 0.7444 - accuracy: 0.7391\n",
            "Epoch 00005: val_accuracy did not improve from 0.75521\n",
            "1219/1219 [==============================] - 348s 285ms/step - loss: 0.7444 - accuracy: 0.7391 - val_loss: 0.7990 - val_accuracy: 0.7396\n",
            "Epoch 6/10\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 0.7323 - accuracy: 0.7452\n",
            "Epoch 00006: val_accuracy did not improve from 0.75521\n",
            "1219/1219 [==============================] - 345s 283ms/step - loss: 0.7323 - accuracy: 0.7452 - val_loss: 2.8962 - val_accuracy: 0.5625\n",
            "Epoch 7/10\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 0.6875 - accuracy: 0.7388\n",
            "Epoch 00007: val_accuracy did not improve from 0.75521\n",
            "1219/1219 [==============================] - 343s 282ms/step - loss: 0.6875 - accuracy: 0.7388 - val_loss: 0.6106 - val_accuracy: 0.7292\n",
            "Epoch 8/10\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 0.6218 - accuracy: 0.7656\n",
            "Epoch 00008: val_accuracy did not improve from 0.75521\n",
            "1219/1219 [==============================] - 342s 280ms/step - loss: 0.6218 - accuracy: 0.7656 - val_loss: 1.1959 - val_accuracy: 0.7552\n",
            "Epoch 9/10\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 0.6421 - accuracy: 0.7532\n",
            "Epoch 00009: val_accuracy did not improve from 0.75521\n",
            "1219/1219 [==============================] - 342s 280ms/step - loss: 0.6421 - accuracy: 0.7532 - val_loss: 0.8540 - val_accuracy: 0.7448\n",
            "Epoch 10/10\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 0.7053 - accuracy: 0.7484\n",
            "Epoch 00010: val_accuracy did not improve from 0.75521\n",
            "1219/1219 [==============================] - 347s 284ms/step - loss: 0.7053 - accuracy: 0.7484 - val_loss: 10.3287 - val_accuracy: 0.7188\n",
            "The best model: /content/drive/MyDrive/MURA-v1.2/output_separate//dense169-WRIST-03-0.76.hdf5\n",
            "Images to be prediced:  460\n",
            "Predicting ...\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 460 batches). You may need to use the repeat() function when building your dataset.\n",
            "Done!\n",
            "[[1.5469202e-01]\n",
            " [2.1014078e-01]\n",
            " [4.7886759e-01]\n",
            " [1.6647494e-01]\n",
            " [6.5839998e-03]\n",
            " [4.2103063e-02]\n",
            " [0.0000000e+00]\n",
            " [0.0000000e+00]\n",
            " [0.0000000e+00]\n",
            " [2.9872701e-04]]\n",
            "['abnormal/XR_WRIST_patient11185_study1_positive_image1.png', 'abnormal/XR_WRIST_patient11185_study1_positive_image2.png', 'abnormal/XR_WRIST_patient11185_study1_positive_image3.png', 'abnormal/XR_WRIST_patient11185_study1_positive_image4.png', 'abnormal/XR_WRIST_patient11186_study1_positive_image1.png', 'abnormal/XR_WRIST_patient11186_study1_positive_image2.png', 'abnormal/XR_WRIST_patient11186_study2_positive_image1.png', 'abnormal/XR_WRIST_patient11186_study2_positive_image2.png', 'abnormal/XR_WRIST_patient11186_study2_positive_image3.png', 'abnormal/XR_WRIST_patient11186_study3_positive_image1.png']\n",
            "460\n",
            "abnormal/XR_WRIST_patient11185_study1_positive_image1.png\n",
            "/content/drive/MyDrive/MURA-v1.2/output_separate/predictions_WRIST.csv Created!\n",
            "In model evaluation.\n",
            "Loading Model\n",
            "Model Loaded!\n",
            "{'abnormal': 0, 'normal': 1}\n",
            "Predicting 460 files.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 460.0 batches). You may need to use the repeat() function when building your dataset.\n",
            "It took:  5.506609201431274\n",
            "pred, shape (460,)\n",
            "/content/drive/MyDrive/MURA-v1.2/temp_WRIST/valid_data/abnormal/XR_WRIST_patient11185_study1_positive_image1.png\n",
            "lens 460 460 460\n",
            "===== WRIST ======\n",
            "roc_auc_score:  0.7688823484443085\n",
            "Sensitivity:  0.9212598425196851\n",
            "Specificity:  0.616504854368932\n",
            "Cohen-Cappa-Score:  0.5527225583405359\n",
            "F1 Score:  0.8253968253968255\n",
            "All WRIST files moved back to their original location.\n",
            "All temporal directories removed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9qVVE2R8h10"
      },
      "source": [
        "# body_part=\"HAND\"\n",
        "# for root, dirs, files in os.walk(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part, topdown=True):\n",
        "#   for file in files:\n",
        "#     if body_part in file:\n",
        "#       if \"valid\" in root:\n",
        "#         if \"abnormal\" in root:\n",
        "#           os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/valid_data/abnormal/\"+file)\n",
        "#         else:\n",
        "#           os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/valid_data/normal/\"+file)\n",
        "#       else:\n",
        "#         if \"abnormal\" in root:\n",
        "#           os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/train_data/abnormal/\"+file)\n",
        "#         else:\n",
        "#           os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/train_data/normal/\"+file)\n",
        "# print(\"All \"+body_part+\" files moved back to their original location.\")\n",
        "\n",
        "\n",
        "# os.rmdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/train_data/abnormal\")\n",
        "# os.rmdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/train_data/normal\")\n",
        "# os.rmdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/valid_data/abnormal\")\n",
        "# os.rmdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/valid_data/normal\")\n",
        "# os.rmdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/train_data\")\n",
        "# os.rmdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/valid_data\")\n",
        "# os.rmdir(\"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part)\n",
        "# print(\"All temporal directories removed.\")\n",
        "\n",
        "#         # if \"valid_data\" in root:\n",
        "#         #   pass\n",
        "#         # #  if \"abnormal\" in root:\n",
        "#         # #    os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/valid_data/abnormal/\"+file)\n",
        "#         #  # elif \"normal\" in root:\n",
        "#         #  #   os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/valid_data/normal/\"+file)\n",
        "#         # elif \"train_data\" in root:\n",
        "#         #   #print(\"ON\")\n",
        "#         #   if \"normal\" in root:\n",
        "#         #     print(\"JOO\")\n",
        "#         #     os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/temp_\"+\"FOREARM\"+\"/train_data/normal/\"+file)\n",
        "#              # elif \"normal\" in root:\n",
        "#           #   os.rename(root+'/'+file, \"/content/drive/MyDrive/MURA-v1.2/temp_\"+body_part+\"/train_data/normal/\"+file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}